{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from warnings import warn\n",
    "\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import ClassificationMetric, BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.postprocessing.reject_option_classification\\\n",
    "        import RejectOptionClassification\n",
    "from common_utils import compute_metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive, FloatSlider\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huangrui's Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_continuous(df,collist,Nlevel):\n",
    "    for col in collist:\n",
    "        for q in range(1,Nlevel,1):\n",
    "            threshold = df[~np.isnan(df[col])][col].quantile(float(q)/Nlevel)\n",
    "            df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
    "    df.drop(collist,axis = 1, inplace = True)\n",
    "    \n",
    "class LawSchoolPassBarDataset(StandardDataset):\n",
    "    \"\"\"Law School PassBar dataset.\n",
    "\n",
    "    See https://github.com/microsoft/tempeh for details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, label_name='pass_bar', favorable_classes=[1],  \n",
    "                 protected_attribute_names=['race'],\n",
    "                 privileged_classes=[[1]],\n",
    "                 instance_weights_name=None,\n",
    "                 categorical_features=[],\n",
    "                 features_to_drop=[],\n",
    "                 features_to_keep=[],\n",
    "                 na_values=[], custom_preprocessing=None,\n",
    "                 metadata=None):\n",
    "        \"\"\"See :obj:`RegressionDataset` for a description of the arguments.\"\"\"\n",
    "        df = pd.read_csv(path)\n",
    "        numericals = [col for col in df.columns if len(df[col].unique())>2 and col not in ['pass_bar']]\n",
    "        code_continuous(df,numericals, 5)\n",
    "        \n",
    "\n",
    "        super(LawSchoolPassBarDataset, self).__init__(\n",
    "            df=df, label_name=label_name,\n",
    "            favorable_classes=favorable_classes,\n",
    "            protected_attribute_names=protected_attribute_names,\n",
    "            privileged_classes=privileged_classes,\n",
    "            instance_weights_name=instance_weights_name,\n",
    "            categorical_features=categorical_features,\n",
    "            features_to_keep=features_to_keep,\n",
    "            features_to_drop=features_to_drop, na_values=na_values,\n",
    "            custom_preprocessing=custom_preprocessing, metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and specify options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "privileged_groups = [{'race': 1}]\n",
    "unprivileged_groups = [{'race': 0}]\n",
    "\n",
    "# Metric used (should be one of allowed_metrics)\n",
    "metric_name = \"Equal opportunity difference\"\n",
    "        \n",
    "#random seed for calibrated equal odds prediction\n",
    "random_seed = 12345679\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Verify metric name\n",
    "allowed_metrics = [\"Statistical parity difference\",\n",
    "                   \"Average odds difference\",\n",
    "                   \"Equal opportunity difference\"]\n",
    "if metric_name not in allowed_metrics:\n",
    "    raise ValueError(\"Metric name should be one of allowed metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0491891064871481\n",
      "0.05163974547234459\n",
      "Balanced accuracy = 0.5000\n",
      "Statistical parity difference = 0.0000\n",
      "Disparate impact = 1.0000\n",
      "Average odds difference = 0.0000\n",
      "Equal opportunity difference = 0.0000\n",
      "Theil index = 0.0177\n",
      "Balanced accuracy = 0.5000\n",
      "Statistical parity difference = 0.0000\n",
      "Disparate impact = 1.0000\n",
      "Average odds difference = 0.0000\n",
      "Equal opportunity difference = 0.0000\n",
      "Theil index = 0.0170\n"
     ]
    }
   ],
   "source": [
    "K = 2\n",
    "\n",
    "dataset_orig_train= LawSchoolPassBarDataset(path=\"./Huangrui/law/law_train{}.csv\".format(K))\n",
    "dataset_orig_test= LawSchoolPassBarDataset(path=\"./Huangrui/law/law_test{}.csv\".format(K))\n",
    "\n",
    "import pickle\n",
    "from sklearn.linear_model import Lasso\n",
    "# 用部分 data训练 biased model\n",
    "selected_dataset,_ = dataset_orig_train.split([0.8], shuffle=False)\n",
    "_ =  dataset_orig_test\n",
    "# Logistic regression classifier and predictions\n",
    "X_train = selected_dataset.features\n",
    "y_train = selected_dataset.labels.ravel()\n",
    "X_test = _.features\n",
    "y_test = _.labels.ravel()\n",
    "lmod = Lasso(alpha = 0.001)\n",
    "lmod.fit(X_train, y_train)\n",
    "y_train_pred = lmod.predict(X_train)>0.5\n",
    "y_test_pred = lmod.predict(X_test)>0.5\n",
    "dataset_orig_train_pred = selected_dataset.copy(deepcopy=True)\n",
    "dataset_orig_train_pred.labels = y_train_pred\n",
    "dataset_orig_test_pred = _.copy(deepcopy=True)\n",
    "dataset_orig_test_pred.labels = y_test_pred\n",
    "print(np.mean(y_train_pred!=y_train))\n",
    "print(np.mean(y_test_pred!=y_test))\n",
    "metric_test = compute_metrics(_, dataset_orig_test_pred, \n",
    "                unprivileged_groups, privileged_groups)\n",
    "metric_train = compute_metrics(selected_dataset, dataset_orig_train_pred,unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save model\n",
    "# bmodels = {}\n",
    "# bmodels['clf'] = lmod\n",
    "# pickle.dump(bmodels, open(\"experiments/german\"+str(K)+'_age_bmodel.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16341, 57)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['race']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.])] [array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['race', 'fulltime', 'parttime', 'sex', 'dropout', 'decile1b_geq1q3.0', 'decile1b_geq2q5.0', 'decile1b_geq3q7.0', 'decile1b_geq4q9.0', 'decile3_geq1q3.0', 'decile3_geq2q5.0', 'decile3_geq3q7.0', 'decile3_geq4q9.0', 'decile1_geq1q3.0', 'decile1_geq2q5.0', 'decile1_geq3q7.0', 'decile1_geq4q9.0', 'cluster_geq1q1.0', 'cluster_geq2q3.0', 'cluster_geq3q3.0', 'cluster_geq4q4.0', 'lsat_geq1q32.5', 'lsat_geq2q36.0', 'lsat_geq3q38.0', 'lsat_geq4q41.0', 'ugpa_geq1q2.9', 'ugpa_geq2q3.1', 'ugpa_geq3q3.4', 'ugpa_geq4q3.6', 'zfygpa_geq1q-0.7', 'zfygpa_geq2q-0.16', 'zfygpa_geq3q0.32', 'zfygpa_geq4q0.89', 'DOB_yr_geq1q63.0', 'DOB_yr_geq2q67.0', 'DOB_yr_geq3q68.0', 'DOB_yr_geq4q69.0', 'zgpa_geq1q-0.85', 'zgpa_geq2q-0.25', 'zgpa_geq3q0.28', 'zgpa_geq4q0.87', 'fam_inc_geq1q3.0', 'fam_inc_geq2q3.0', 'fam_inc_geq3q4.0', 'fam_inc_geq4q4.0', 'age_geq1q-62.0', 'age_geq2q-61.0', 'age_geq3q-60.0', 'age_geq4q-57.0', 'tier_geq1q3.0', 'tier_geq2q3.0', 'tier_geq3q4.0', 'tier_geq4q5.0', 'index6040_geq1q663.157879', 'index6040_geq2q720.526298', 'index6040_geq3q772.1052440000001', 'index6040_geq4q831.052609']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig_train.privileged_protected_attributes, \n",
    "      dataset_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
