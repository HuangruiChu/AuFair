{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook demonstrates the use of the Reject Option Classification (ROC) post-processing algorithm for bias mitigation.\n",
    "- The debiasing function used is implemented in the `RejectOptionClassification` class.\n",
    "- Divide the dataset into training, validation, and testing partitions.\n",
    "- Train classifier on original training data.\n",
    "- Estimate the optimal classification threshold, that maximizes balanced accuracy without fairness constraints.\n",
    "- Estimate the optimal classification threshold, and the critical region boundary (ROC margin) using a validation set for the desired constraint on fairness. The best parameters are those that maximize the classification threshold while satisfying the fairness constraints.\n",
    "- The constraints can be used on the following fairness measures:\n",
    "    * Statistical parity difference on the predictions of the classifier\n",
    "    * Average odds difference for the classifier\n",
    "    * Equal opportunity difference for the classifier\n",
    "- Determine the prediction scores for testing data. Using the estimated optimal classification threshold, compute accuracy and fairness metrics.\n",
    "- Using the determined optimal classification threshold and the ROC margin, adjust the predictions. Report accuracy and fairness metric on the new predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from warnings import warn\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import ClassificationMetric, BinaryLabelDatasetMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "        import load_preproc_data_adult, load_preproc_data_german, load_preproc_data_compas\n",
    "from aif360.algorithms.postprocessing.reject_option_classification\\\n",
    "        import RejectOptionClassification\n",
    "from common_utils import compute_metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive, FloatSlider\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huangrui's Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_continuous(df,collist,Nlevel):\n",
    "    for col in collist:\n",
    "        for q in range(1,Nlevel,1):\n",
    "            threshold = df[~np.isnan(df[col])][col].quantile(float(q)/Nlevel)\n",
    "            df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
    "    df.drop(collist,axis = 1, inplace = True)\n",
    "    \n",
    "class BankDataset(StandardDataset):\n",
    "    \"\"\"financial-inclusion-in-africa dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, label_name='y', favorable_classes=[1],  \n",
    "                 protected_attribute_names=['age'],\n",
    "                 privileged_classes=[[1]],\n",
    "                 instance_weights_name=None,\n",
    "                 categorical_features=[],\n",
    "                 features_to_drop=[],\n",
    "                 features_to_keep=[],\n",
    "                 na_values=[], custom_preprocessing=None,\n",
    "                 metadata=None):\n",
    "        \"\"\"See :obj:`RegressionDataset` for a description of the arguments.\"\"\"\n",
    "        \n",
    "\n",
    "        df = pd.read_csv(path)\n",
    "        df[\"cons.conf.idx\"] = -df[\"cons.conf.idx\"]\n",
    "        numericals = [col for col in df.columns if len(df[col].unique())>2 and max(df[col])>1]\n",
    "        code_continuous(df,numericals, 5)\n",
    "\n",
    "        super(BankDataset, self).__init__(\n",
    "            df=df, label_name=label_name,\n",
    "            favorable_classes=favorable_classes,\n",
    "            protected_attribute_names=protected_attribute_names,\n",
    "            privileged_classes=privileged_classes,\n",
    "            instance_weights_name=instance_weights_name,\n",
    "            categorical_features=categorical_features,\n",
    "            features_to_keep=features_to_keep,\n",
    "            features_to_drop=features_to_drop, na_values=na_values,\n",
    "            custom_preprocessing=custom_preprocessing, metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and specify options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]\n",
    "\n",
    "# Metric used (should be one of allowed_metrics)\n",
    "metric_name = \"Equal opportunity difference\"\n",
    "        \n",
    "#random seed for calibrated equal odds prediction\n",
    "random_seed = 12345679\n",
    "np.random.seed(random_seed)\n",
    "# Upper and lower bound on the fairness metric used\n",
    "metric_ub = 0.05\n",
    "metric_lb = -0.05\n",
    "# Verify metric name\n",
    "allowed_metrics = [\"Statistical parity difference\",\n",
    "                   \"Average odds difference\",\n",
    "                   \"Equal opportunity difference\"]\n",
    "if metric_name not in allowed_metrics:\n",
    "    raise ValueError(\"Metric name should be one of allowed metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.8191\n",
      "Statistical parity difference = 0.3088\n",
      "Disparate impact = 1.9426\n",
      "Average odds difference = 0.1899\n",
      "Equal opportunity difference = 0.0909\n",
      "Theil index = 0.0626\n",
      "Balanced accuracy = 0.8242\n",
      "Statistical parity difference = 0.4216\n",
      "Disparate impact = 2.0632\n",
      "Average odds difference = 0.2222\n",
      "Equal opportunity difference = 0.0000\n",
      "Theil index = 0.0583\n",
      "Balanced accuracy = 0.7788\n",
      "Statistical parity difference = 0.2218\n",
      "Disparate impact = 1.5046\n",
      "Average odds difference = 0.1068\n",
      "Equal opportunity difference = 0.0287\n",
      "Theil index = 0.0675\n",
      "Balanced accuracy = 0.7582\n",
      "Statistical parity difference = 0.4338\n",
      "Disparate impact = 1.8466\n",
      "Average odds difference = 0.2530\n",
      "Equal opportunity difference = 0.0259\n",
      "Theil index = 0.0627\n",
      "K = 1, budget = 0.01\n",
      "The Error for the test dataset is 0.4035\n",
      "The Equal opportunity difference for the test dataset is 0.02589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\edb\\languagepack\\v2\\Python-3.9\\lib\\site-packages\\aif360\\metrics\\classification_metric.py:278: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TPR=TP / P, TNR=TN / N, FPR=FP / N, FNR=FN / P,\n",
      "C:\\edb\\languagepack\\v2\\Python-3.9\\lib\\site-packages\\aif360\\metrics\\classification_metric.py:279: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  GTPR=GTP / P, GTNR=GTN / N, GFPR=GFP / N, GFNR=GFN / P,\n",
      "C:\\edb\\languagepack\\v2\\Python-3.9\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\reject_option_classification.py:160: UserWarning: Unable to satisy fairness constraints\n",
      "  warn(\"Unable to satisy fairness constraints\")\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [28], line 60\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# print(\"Best balanced accuracy (no fairness constraints) = %.4f\" % np.max(ba_arr))\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# print(\"Optimal classification threshold (no fairness constraints) = %.4f\" % best_class_thresh)\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m#### Estimate optimal parameters for the ROC method\u001b[39;00m\n\u001b[0;32m     54\u001b[0m ROC \u001b[38;5;241m=\u001b[39m RejectOptionClassification(unprivileged_groups\u001b[38;5;241m=\u001b[39munprivileged_groups, \n\u001b[0;32m     55\u001b[0m                                 privileged_groups\u001b[38;5;241m=\u001b[39mprivileged_groups, \n\u001b[0;32m     56\u001b[0m                                 low_class_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, high_class_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m,\n\u001b[0;32m     57\u001b[0m                                 num_class_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, num_ROC_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     58\u001b[0m                                 metric_name\u001b[38;5;241m=\u001b[39mmetric_name,\n\u001b[0;32m     59\u001b[0m                                 metric_ub\u001b[38;5;241m=\u001b[39mmetric_ub, metric_lb\u001b[38;5;241m=\u001b[39mmetric_lb)\n\u001b[1;32m---> 60\u001b[0m ROC \u001b[38;5;241m=\u001b[39m \u001b[43mROC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_orig_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_orig_train_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC.classification_threshold)\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Metrics for the train set\u001b[39;00m\n\u001b[0;32m     65\u001b[0m fav_inds \u001b[38;5;241m=\u001b[39m dataset_orig_train_pred\u001b[38;5;241m.\u001b[39mscores \u001b[38;5;241m>\u001b[39m best_class_thresh\n",
      "File \u001b[1;32mC:\\edb\\languagepack\\v2\\Python-3.9\\lib\\site-packages\\aif360\\algorithms\\transformer.py:27\u001b[0m, in \u001b[0;36maddmetadata.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 27\u001b[0m     new_dataset \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_dataset, Dataset):\n\u001b[0;32m     29\u001b[0m         new_dataset\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m new_dataset\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mC:\\edb\\languagepack\\v2\\Python-3.9\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\reject_option_classification.py:162\u001b[0m, in \u001b[0;36mRejectOptionClassification.fit\u001b[1;34m(self, dataset_true, dataset_pred)\u001b[0m\n\u001b[0;32m    160\u001b[0m     warn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to satisy fairness constraints\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    161\u001b[0m     rel_inds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(fair_metric_arr), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m--> 162\u001b[0m     best_ind \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfair_metric_arr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrel_inds\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfair_metric_arr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrel_inds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mROC_margin \u001b[38;5;241m=\u001b[39m ROC_margin_arr[rel_inds][best_ind]\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification_threshold \u001b[38;5;241m=\u001b[39m class_thresh_arr[rel_inds][best_ind]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "experiments_info = {}\n",
    "budget = 0.01\n",
    "for K in range(1, 6):\n",
    "    dataset_orig_train= BankDataset(path=\"./Huangrui/bank/bank_train{}.csv\".format(K))\n",
    "    dataset_orig_test= BankDataset(path=\"./Huangrui/bank/bank_test{}.csv\".format(K))\n",
    "    #only use the budget% of the training data\n",
    "    # Get the dataset and split into train and test\n",
    "    if budget == 0.01:\n",
    "        dataset_orig_train, _ = dataset_orig_train.split([0.01], shuffle=True,seed = 12345679)\n",
    "    else:\n",
    "        dataset_orig_train,_ = dataset_orig_train.split([budget], shuffle=False)\n",
    "\n",
    "    # Lasso linear classifier and predictions\n",
    "    X_train = dataset_orig_train.features\n",
    "    y_train = dataset_orig_train.labels.ravel()\n",
    "    lmod = pickle.load(open('experiments/bank'+str(K)+'_age_bmodel.pkl','rb'))[\"clf\"]\n",
    "   \n",
    "    y_train_pred = lmod.predict(X_train)\n",
    "\n",
    "    dataset_orig_train_pred = dataset_orig_train.copy(deepcopy=True)\n",
    "    dataset_orig_train_pred.labels = y_train_pred.reshape(-1,1)\n",
    "    sigmoid = lambda x: 1 / (1 + np.exp(0.5-x))\n",
    "    dataset_orig_train_pred.scores = sigmoid(y_train_pred).reshape(-1,1)\n",
    "\n",
    "    dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "    X_test = dataset_orig_test_pred.features\n",
    "    y_test = dataset_orig_test_pred.labels\n",
    "    dataset_orig_test_pred.scores = sigmoid(lmod.predict(X_test)).reshape(-1,1)\n",
    "\n",
    "    #### Best threshold for classification only (no fairness)\n",
    "    num_thresh = 100\n",
    "    ba_arr = np.zeros(num_thresh)\n",
    "    class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "    for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "        \n",
    "        fav_inds = dataset_orig_train_pred.scores > class_thresh\n",
    "        dataset_orig_train_pred.labels[fav_inds] = dataset_orig_train_pred.favorable_label\n",
    "        dataset_orig_train_pred.labels[~fav_inds] = dataset_orig_train_pred.unfavorable_label\n",
    "        \n",
    "        classified_metric_orig_train = ClassificationMetric(dataset_orig_train,\n",
    "                                                dataset_orig_train_pred, \n",
    "                                                unprivileged_groups=unprivileged_groups,\n",
    "                                                privileged_groups=privileged_groups)\n",
    "        \n",
    "        ba_arr[idx] = 0.5*(classified_metric_orig_train.true_positive_rate()\\\n",
    "                        +classified_metric_orig_train.true_negative_rate())\n",
    "\n",
    "    best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "    best_class_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "    # print(\"Best balanced accuracy (no fairness constraints) = %.4f\" % np.max(ba_arr))\n",
    "    # print(\"Optimal classification threshold (no fairness constraints) = %.4f\" % best_class_thresh)\n",
    "    #### Estimate optimal parameters for the ROC method\n",
    "    ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                    privileged_groups=privileged_groups, \n",
    "                                    low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                    num_class_thresh=100, num_ROC_margin=50,\n",
    "                                    metric_name=metric_name,\n",
    "                                    metric_ub=metric_ub, metric_lb=metric_lb)\n",
    "    ROC = ROC.fit(dataset_orig_train, dataset_orig_train_pred)\n",
    "    # print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC.classification_threshold)\n",
    "    # print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)\n",
    "\n",
    "    # Metrics for the train set\n",
    "    fav_inds = dataset_orig_train_pred.scores > best_class_thresh\n",
    "    dataset_orig_train_pred.labels[fav_inds] = dataset_orig_train_pred.favorable_label\n",
    "    dataset_orig_train_pred.labels[~fav_inds] = dataset_orig_train_pred.unfavorable_label\n",
    "\n",
    "    # display(Markdown(\"#### train set\"))\n",
    "    # display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "    metric_train_bef = compute_metrics(dataset_orig_train, dataset_orig_train_pred, \n",
    "                    unprivileged_groups, privileged_groups)\n",
    "\n",
    "    # Transform the validation set\n",
    "    dataset_transf_train_pred = ROC.predict(dataset_orig_train_pred)\n",
    "\n",
    "    # display(Markdown(\"#### train set\"))\n",
    "    # display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "    metric_train_aft = compute_metrics(dataset_orig_train, dataset_transf_train_pred, \n",
    "                    unprivileged_groups, privileged_groups)\n",
    "    # Testing: Check if the metric optimized has not become worse\n",
    "    assert np.abs(metric_train_aft[metric_name]) <= np.abs(metric_train_bef[metric_name])\n",
    "\n",
    "    # Metrics for the test set\n",
    "    fav_inds = dataset_orig_test_pred.scores > best_class_thresh\n",
    "    dataset_orig_test_pred.labels[fav_inds] = dataset_orig_test_pred.favorable_label\n",
    "    dataset_orig_test_pred.labels[~fav_inds] = dataset_orig_test_pred.unfavorable_label\n",
    "\n",
    "    # display(Markdown(\"#### Test set\"))\n",
    "    # display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "    metric_test_bef = compute_metrics(dataset_orig_test, dataset_orig_test_pred, \n",
    "                    unprivileged_groups, privileged_groups)\n",
    "    # Metrics for the transformed test set\n",
    "    dataset_transf_test_pred = ROC.predict(dataset_orig_test_pred)\n",
    "\n",
    "    # display(Markdown(\"#### Test set\"))\n",
    "    # display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "    metric_test_aft = compute_metrics(dataset_orig_test, dataset_transf_test_pred, \n",
    "                    unprivileged_groups, privileged_groups)\n",
    "\n",
    "    #自己计算error, 不是balanced accuracy！！！\n",
    "    print(\"K = {}, budget = {}\".format(K, budget))\n",
    "    print(\"The Error for the test dataset is {:.4}\".format(np.mean(dataset_orig_test.labels!=dataset_transf_test_pred.labels)))\n",
    "    print(\"The Equal opportunity difference for the test dataset is {:.4}\".format(metric_test_aft[\"Equal opportunity difference\"]))\n",
    "    experiments_info[\"K = {}, budget = {}\".format(K, budget)] = {\"Error\": np.mean(dataset_orig_test.labels!=dataset_transf_test_pred.labels), \"Equal opportunity difference\": metric_test_aft[\"Equal opportunity difference\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For budget = 0.01 We need to do it seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.9107\n",
      "Statistical parity difference = -0.0639\n",
      "Disparate impact = 0.7400\n",
      "Average odds difference = -0.0411\n",
      "Equal opportunity difference = 0.0000\n",
      "Theil index = 0.0436\n",
      "Balanced accuracy = 0.9107\n",
      "Statistical parity difference = -0.0639\n",
      "Disparate impact = 0.7400\n",
      "Average odds difference = -0.0411\n",
      "Equal opportunity difference = 0.0000\n",
      "Theil index = 0.0436\n",
      "Balanced accuracy = 0.8245\n",
      "Statistical parity difference = 0.2034\n",
      "Disparate impact = 1.6793\n",
      "Average odds difference = 0.1225\n",
      "Equal opportunity difference = 0.1062\n",
      "Theil index = 0.0644\n",
      "Balanced accuracy = 0.8245\n",
      "Statistical parity difference = 0.2034\n",
      "Disparate impact = 1.6793\n",
      "Average odds difference = 0.1225\n",
      "Equal opportunity difference = 0.1062\n",
      "Theil index = 0.0644\n",
      "K = 5, budget = 0.01\n",
      "The Error for the test dataset is 0.2132\n",
      "The Equal opportunity difference for the test dataset is 0.1062\n"
     ]
    }
   ],
   "source": [
    "dataset_orig_train= BankDataset(path=\"./Huangrui/bank/bank_train{}.csv\".format(K))\n",
    "dataset_orig_test= BankDataset(path=\"./Huangrui/bank/bank_test{}.csv\".format(K))\n",
    "K = 5\n",
    "#only use the budget% of the training data\n",
    "# Get the dataset and split into train and test\n",
    "budget =0.01\n",
    "   \n",
    "dataset_orig_train,_ = dataset_orig_train.split([budget], shuffle=True)\n",
    "\n",
    "# Lasso linear classifier and predictions\n",
    "X_train = dataset_orig_train.features\n",
    "y_train = dataset_orig_train.labels.ravel()\n",
    "lmod = pickle.load(open('experiments/bank'+str(K)+'_age_bmodel.pkl','rb'))[\"clf\"]\n",
    "\n",
    "y_train_pred = lmod.predict(X_train)\n",
    "\n",
    "dataset_orig_train_pred = dataset_orig_train.copy(deepcopy=True)\n",
    "dataset_orig_train_pred.labels = y_train_pred.reshape(-1,1)\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(0.5-x))\n",
    "dataset_orig_train_pred.scores = sigmoid(y_train_pred).reshape(-1,1)\n",
    "\n",
    "dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "X_test = dataset_orig_test_pred.features\n",
    "y_test = dataset_orig_test_pred.labels\n",
    "dataset_orig_test_pred.scores = sigmoid(lmod.predict(X_test)).reshape(-1,1)\n",
    "\n",
    "#### Best threshold for classification only (no fairness)\n",
    "num_thresh = 100\n",
    "ba_arr = np.zeros(num_thresh)\n",
    "class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "    \n",
    "    fav_inds = dataset_orig_train_pred.scores > class_thresh\n",
    "    dataset_orig_train_pred.labels[fav_inds] = dataset_orig_train_pred.favorable_label\n",
    "    dataset_orig_train_pred.labels[~fav_inds] = dataset_orig_train_pred.unfavorable_label\n",
    "    \n",
    "    classified_metric_orig_train = ClassificationMetric(dataset_orig_train,\n",
    "                                            dataset_orig_train_pred, \n",
    "                                            unprivileged_groups=unprivileged_groups,\n",
    "                                            privileged_groups=privileged_groups)\n",
    "    \n",
    "    ba_arr[idx] = 0.5*(classified_metric_orig_train.true_positive_rate()\\\n",
    "                    +classified_metric_orig_train.true_negative_rate())\n",
    "\n",
    "best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "best_class_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "# print(\"Best balanced accuracy (no fairness constraints) = %.4f\" % np.max(ba_arr))\n",
    "# print(\"Optimal classification threshold (no fairness constraints) = %.4f\" % best_class_thresh)\n",
    "#### Estimate optimal parameters for the ROC method\n",
    "ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                privileged_groups=privileged_groups, \n",
    "                                low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                num_class_thresh=100, num_ROC_margin=50,\n",
    "                                metric_name=metric_name,\n",
    "                                metric_ub=metric_ub, metric_lb=metric_lb)\n",
    "ROC = ROC.fit(dataset_orig_train, dataset_orig_train_pred)\n",
    "# print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC.classification_threshold)\n",
    "# print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)\n",
    "\n",
    "# Metrics for the train set\n",
    "fav_inds = dataset_orig_train_pred.scores > best_class_thresh\n",
    "dataset_orig_train_pred.labels[fav_inds] = dataset_orig_train_pred.favorable_label\n",
    "dataset_orig_train_pred.labels[~fav_inds] = dataset_orig_train_pred.unfavorable_label\n",
    "\n",
    "# display(Markdown(\"#### train set\"))\n",
    "# display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "metric_train_bef = compute_metrics(dataset_orig_train, dataset_orig_train_pred, \n",
    "                unprivileged_groups, privileged_groups)\n",
    "\n",
    "# Transform the validation set\n",
    "dataset_transf_train_pred = ROC.predict(dataset_orig_train_pred)\n",
    "\n",
    "# display(Markdown(\"#### train set\"))\n",
    "# display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "metric_train_aft = compute_metrics(dataset_orig_train, dataset_transf_train_pred, \n",
    "                unprivileged_groups, privileged_groups)\n",
    "# Testing: Check if the metric optimized has not become worse\n",
    "assert np.abs(metric_train_aft[metric_name]) <= np.abs(metric_train_bef[metric_name])\n",
    "\n",
    "# Metrics for the test set\n",
    "fav_inds = dataset_orig_test_pred.scores > best_class_thresh\n",
    "dataset_orig_test_pred.labels[fav_inds] = dataset_orig_test_pred.favorable_label\n",
    "dataset_orig_test_pred.labels[~fav_inds] = dataset_orig_test_pred.unfavorable_label\n",
    "\n",
    "# display(Markdown(\"#### Test set\"))\n",
    "# display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "metric_test_bef = compute_metrics(dataset_orig_test, dataset_orig_test_pred, \n",
    "                unprivileged_groups, privileged_groups)\n",
    "# Metrics for the transformed test set\n",
    "dataset_transf_test_pred = ROC.predict(dataset_orig_test_pred)\n",
    "\n",
    "# display(Markdown(\"#### Test set\"))\n",
    "# display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "metric_test_aft = compute_metrics(dataset_orig_test, dataset_transf_test_pred, \n",
    "                unprivileged_groups, privileged_groups)\n",
    "\n",
    "#自己计算error, 不是balanced accuracy！！！\n",
    "print(\"K = {}, budget = {}\".format(K, budget))\n",
    "print(\"The Error for the test dataset is {:.4}\".format(np.mean(dataset_orig_test.labels!=dataset_transf_test_pred.labels)))\n",
    "print(\"The Equal opportunity difference for the test dataset is {:.4}\".format(metric_test_aft[\"Equal opportunity difference\"]))\n",
    "experiments_info[\"K = {}, budget = {}\".format(K, budget)] = {\"Error\": np.mean(dataset_orig_test.labels!=dataset_transf_test_pred.labels), \"Equal opportunity difference\": metric_test_aft[\"Equal opportunity difference\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'K = 1, budget = 0.01': {'Error': 0.3007543456871105,\n",
       "  'Equal opportunity difference': 0.031039136302294157},\n",
       " 'K = 2, budget = 0.01': {'Error': 0.4415286206330982,\n",
       "  'Equal opportunity difference': 0.00544959128065392},\n",
       " 'K = 3, budget = 0.01': {'Error': 0.42850114791734994,\n",
       "  'Equal opportunity difference': 0.013495276653171406},\n",
       " 'K = 4, budget = 0.01': {'Error': 0.25237782879632664,\n",
       "  'Equal opportunity difference': 0.0625},\n",
       " 'K = 5, budget = 0.01': {'Error': 0.21321961620469082,\n",
       "  'Equal opportunity difference': 0.1061706629055007}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
