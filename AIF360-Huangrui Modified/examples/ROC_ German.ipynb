{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook demonstrates the use of the Reject Option Classification (ROC) post-processing algorithm for bias mitigation.\n",
    "- The debiasing function used is implemented in the `RejectOptionClassification` class.\n",
    "- Divide the dataset into training, validation, and testing partitions.\n",
    "- Train classifier on original training data.\n",
    "- Estimate the optimal classification threshold, that maximizes balanced accuracy without fairness constraints.\n",
    "- Estimate the optimal classification threshold, and the critical region boundary (ROC margin) using a validation set for the desired constraint on fairness. The best parameters are those that maximize the classification threshold while satisfying the fairness constraints.\n",
    "- The constraints can be used on the following fairness measures:\n",
    "    * Statistical parity difference on the predictions of the classifier\n",
    "    * Average odds difference for the classifier\n",
    "    * Equal opportunity difference for the classifier\n",
    "- Determine the prediction scores for testing data. Using the estimated optimal classification threshold, compute accuracy and fairness metrics.\n",
    "- Using the determined optimal classification threshold and the ROC margin, adjust the predictions. Report accuracy and fairness metric on the new predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from warnings import warn\n",
    "\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import ClassificationMetric, BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.postprocessing.reject_option_classification\\\n",
    "        import RejectOptionClassification\n",
    "from common_utils import compute_metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive, FloatSlider\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huangrui's Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_preproc_data_german(protected_attributes=None):\n",
    "#     \"\"\"\n",
    "#     Load and pre-process german credit dataset.\n",
    "#     Args:\n",
    "#         protected_attributes(list or None): If None use all possible protected\n",
    "#             attributes, else subset the protected attributes to the list.\n",
    "\n",
    "#     Returns:\n",
    "#         GermanDataset: An instance of GermanDataset with required pre-processing.\n",
    "\n",
    "#     \"\"\"\n",
    "#     def custom_preprocessing(df):\n",
    "#         \"\"\" Custom pre-processing for German Credit Data\n",
    "#         \"\"\"\n",
    "\n",
    "#         def group_credit_hist(x):\n",
    "#             if x in ['A30', 'A31', 'A32']:\n",
    "#                 return 'None/Paid'\n",
    "#             elif x == 'A33':\n",
    "#                 return 'Delay'\n",
    "#             elif x == 'A34':\n",
    "#                 return 'Other'\n",
    "#             else:\n",
    "#                 return 'NA'\n",
    "\n",
    "#         def group_employ(x):\n",
    "#             if x == 'A71':\n",
    "#                 return 'Unemployed'\n",
    "#             elif x in ['A72', 'A73']:\n",
    "#                 return '1-4 years'\n",
    "#             elif x in ['A74', 'A75']:\n",
    "#                 return '4+ years'\n",
    "#             else:\n",
    "#                 return 'NA'\n",
    "\n",
    "#         def group_savings(x):\n",
    "#             if x in ['A61', 'A62']:\n",
    "#                 return '<500'\n",
    "#             elif x in ['A63', 'A64']:\n",
    "#                 return '500+'\n",
    "#             elif x == 'A65':\n",
    "#                 return 'Unknown/None'\n",
    "#             else:\n",
    "#                 return 'NA'\n",
    "\n",
    "#         def group_status(x):\n",
    "#             if x in ['A22', 'A22']:\n",
    "#                 return '<200'\n",
    "#             elif x in ['A23']:\n",
    "#                 return '200+'\n",
    "#             elif x == 'A14':\n",
    "#                 return 'None'\n",
    "#             else:\n",
    "#                 return 'NA'\n",
    "\n",
    "#         status_map = {'A91': 1.0, 'A93': 1.0, 'A94': 1.0,\n",
    "#                     'A92': 0.0, 'A95': 0.0}\n",
    "#         df['sex'] = df['personal_status'].replace(status_map)\n",
    "#         #drop personal_status,not leak sex information\n",
    "#         df = df.drop('personal_status', axis=1)\n",
    "\n",
    "#         # group credit history, savings, and employment\n",
    "#         df['credit_history'] = df['credit_history'].apply(lambda x: group_credit_hist(x))\n",
    "#         df['savings'] = df['savings'].apply(lambda x: group_savings(x))\n",
    "#         df['employment'] = df['employment'].apply(lambda x: group_employ(x))\n",
    "#         df['age'] = (df['age'] >= 26).astype(float)\n",
    "#         df['status'] = df['status'].apply(lambda x: group_status(x))\n",
    "#         df[\"Y\"] = df[\"credit\"].apply(lambda x: 1 if x == 1 else 0)\n",
    "#         df.drop([\"credit\"], axis=1, inplace=True)\n",
    "#         return df\n",
    "\n",
    "#     # Feature partitions\n",
    "#     D_features = ['sex', 'age'] if protected_attributes is None else protected_attributes\n",
    "#     Y_features = ['Y']\n",
    "\n",
    "#     # privileged classes\n",
    "#     all_privileged_classes = {\"sex\": [1.0],\n",
    "#                               \"age\": [1.0]}\n",
    "\n",
    "#     # protected attribute maps\n",
    "#     all_protected_attribute_maps = {\"sex\": {1.0: 'Male', 0.0: 'Female'},\n",
    "#                                     \"age\": {1.0: 'Old', 0.0: 'Young'}}\n",
    "\n",
    "#     return GermanDataset(\n",
    "#         label_name=Y_features[0],\n",
    "#         favorable_classes=[1],\n",
    "#         protected_attribute_names=D_features,\n",
    "#         privileged_classes=[all_privileged_classes[x] for x in D_features],\n",
    "#         instance_weights_name=None,\n",
    "#         features_to_keep=[],\n",
    "#         metadata={ 'label_maps': [{1.0: 'Good Credit', 0: 'Bad Credit'}],\n",
    "#                    'protected_attribute_maps': [all_protected_attribute_maps[x]\n",
    "#                                 for x in D_features]},\n",
    "#         custom_preprocessing=custom_preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GermanDataset(StandardDataset):\n",
    "    \"\"\"German credit Dataset.\n",
    "\n",
    "    See :file:`aif360/data/raw/german/README.md`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, label_name='Y', favorable_classes=[1],\n",
    "                 protected_attribute_names=['sex', 'age'],\n",
    "                 privileged_classes=[[1],[1]],\n",
    "                 instance_weights_name=None,\n",
    "                 categorical_features=[],\n",
    "                 features_to_keep=[], features_to_drop=[],\n",
    "                 na_values=[], custom_preprocessing=None,\n",
    "                 metadata=None):\n",
    "        \n",
    "        df = pd.read_csv(path)\n",
    "        \n",
    "\n",
    "        super(GermanDataset, self).__init__(df=df, label_name=label_name,\n",
    "            favorable_classes=favorable_classes,\n",
    "            protected_attribute_names=protected_attribute_names,\n",
    "            privileged_classes=privileged_classes,\n",
    "            instance_weights_name=instance_weights_name,\n",
    "            categorical_features=categorical_features,\n",
    "            features_to_keep=features_to_keep,\n",
    "            features_to_drop=features_to_drop, na_values=na_values,\n",
    "            custom_preprocessing=custom_preprocessing, metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and specify options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import dataset\n",
    "dataset_used = \"german\" # \"german\", \"german\", \"compas\"\n",
    "protected_attribute_used = 2 # 1, 2\n",
    "\n",
    "\n",
    "#     dataset_orig = GermanDataset()\n",
    "if protected_attribute_used == 1:\n",
    "    privileged_groups = [{'sex': 1}]\n",
    "    unprivileged_groups = [{'sex': 0}]\n",
    "else:\n",
    "    privileged_groups = [{'age': 1}]\n",
    "    unprivileged_groups = [{'age': 0}]\n",
    "        \n",
    "# Metric used (should be one of allowed_metrics)\n",
    "metric_name = \"Equal opportunity difference\"\n",
    "\n",
    "# Upper and lower bound on the fairness metric used\n",
    "metric_ub = 0.05\n",
    "metric_lb = -0.05\n",
    "        \n",
    "        \n",
    "#random seed for calibrated equal odds prediction\n",
    "random_seed = 12345679\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Verify metric name\n",
    "allowed_metrics = [\"Statistical parity difference\",\n",
    "                   \"Average odds difference\",\n",
    "                   \"Equal opportunity difference\"]\n",
    "if metric_name not in allowed_metrics:\n",
    "    raise ValueError(\"Metric name should be one of allowed metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5 fold cross validation\n",
    "# Z =  dataset_orig.split(5, shuffle=True,seed = random_seed)\n",
    "# # i th fold\n",
    "# dataset_train1 = Z[0].copy()\n",
    "# dataset_train1.features = np.concatenate((Z[0].features,Z[1].features,Z[2].features,Z[3].features),axis=0)\n",
    "# dataset_train1.scores = np.concatenate((Z[0].scores,Z[1].scores,Z[2].scores,Z[3].scores),axis=0)\n",
    "# dataset_train1.labels = np.concatenate((Z[0].labels,Z[1].labels,Z[2].labels,Z[3].labels),axis=0)\n",
    "# dataset_train1.protected_attributes = np.concatenate((Z[0].protected_attributes,Z[1].protected_attributes,Z[2].protected_attributes,Z[3].protected_attributes),axis=0)\n",
    "# dataset_train1.instance_weights = np.concatenate((Z[0].instance_weights,Z[1].instance_weights,Z[2].instance_weights,Z[3].instance_weights),axis=0)\n",
    "# dataset_train1.instance_names = np.concatenate((Z[0].instance_names,Z[1].instance_names,Z[2].instance_names,Z[3].instance_names),axis=0)\n",
    "# dataset_train1.metadata = Z[0].metadata.copy()\n",
    "# dataset_test1= Z[4].copy()\n",
    "\n",
    "# dataset_train2 = Z[1].copy()\n",
    "# dataset_train2.features = np.concatenate((Z[1].features,Z[2].features,Z[3].features,Z[4].features),axis=0)\n",
    "# dataset_train2.scores = np.concatenate((Z[1].scores,Z[2].scores,Z[3].scores,Z[4].scores),axis=0)\n",
    "# dataset_train2.labels = np.concatenate((Z[1].labels,Z[2].labels,Z[3].labels,Z[4].labels),axis=0)\n",
    "# dataset_train2.protected_attributes = np.concatenate((Z[1].protected_attributes,Z[2].protected_attributes,Z[3].protected_attributes,Z[4].protected_attributes),axis=0)\n",
    "# dataset_train2.instance_weights = np.concatenate((Z[1].instance_weights,Z[2].instance_weights,Z[3].instance_weights,Z[4].instance_weights),axis=0)\n",
    "# dataset_train2.instance_names = np.concatenate((Z[1].instance_names,Z[2].instance_names,Z[3].instance_names,Z[4].instance_names),axis=0)\n",
    "# dataset_train2.metadata = Z[1].metadata.copy()\n",
    "# dataset_test2= Z[0].copy()\n",
    "\n",
    "# dataset_train3 = Z[2].copy()\n",
    "# dataset_train3.features = np.concatenate((Z[2].features,Z[3].features,Z[4].features,Z[0].features),axis=0)\n",
    "# dataset_train3.scores = np.concatenate((Z[2].scores,Z[3].scores,Z[4].scores,Z[0].scores),axis=0)\n",
    "# dataset_train3.labels = np.concatenate((Z[2].labels,Z[3].labels,Z[4].labels,Z[0].labels),axis=0)\n",
    "# dataset_train3.protected_attributes = np.concatenate((Z[2].protected_attributes,Z[3].protected_attributes,Z[4].protected_attributes,Z[0].protected_attributes),axis=0)\n",
    "# dataset_train3.instance_weights = np.concatenate((Z[2].instance_weights,Z[3].instance_weights,Z[4].instance_weights,Z[0].instance_weights),axis=0)\n",
    "# dataset_train3.instance_names = np.concatenate((Z[2].instance_names,Z[3].instance_names,Z[4].instance_names,Z[0].instance_names),axis=0)\n",
    "# dataset_train3.metadata = Z[2].metadata.copy()\n",
    "# dataset_test3= Z[1].copy()\n",
    "\n",
    "# dataset_train4 = Z[3].copy()\n",
    "# dataset_train4.features = np.concatenate((Z[3].features,Z[4].features,Z[0].features,Z[1].features),axis=0)\n",
    "# dataset_train4.scores = np.concatenate((Z[3].scores,Z[4].scores,Z[0].scores,Z[1].scores),axis=0)\n",
    "# dataset_train4.labels = np.concatenate((Z[3].labels,Z[4].labels,Z[0].labels,Z[1].labels),axis=0)\n",
    "# dataset_train4.protected_attributes = np.concatenate((Z[3].protected_attributes,Z[4].protected_attributes,Z[0].protected_attributes,Z[1].protected_attributes),axis=0)\n",
    "# dataset_train4.instance_weights = np.concatenate((Z[3].instance_weights,Z[4].instance_weights,Z[0].instance_weights,Z[1].instance_weights),axis=0)\n",
    "# dataset_train4.instance_names = np.concatenate((Z[3].instance_names,Z[4].instance_names,Z[0].instance_names,Z[1].instance_names),axis=0)\n",
    "# dataset_train4.metadata = Z[3].metadata.copy()\n",
    "# dataset_test4= Z[2].copy()\n",
    "\n",
    "# dataset_train5 = Z[4].copy()\n",
    "# dataset_train5.features = np.concatenate((Z[4].features,Z[0].features,Z[1].features,Z[2].features),axis=0)\n",
    "# dataset_train5.scores = np.concatenate((Z[4].scores,Z[0].scores,Z[1].scores,Z[2].scores),axis=0)\n",
    "# dataset_train5.labels = np.concatenate((Z[4].labels,Z[0].labels,Z[1].labels,Z[2].labels),axis=0)\n",
    "# dataset_train5.protected_attributes = np.concatenate((Z[4].protected_attributes,Z[0].protected_attributes,Z[1].protected_attributes,Z[2].protected_attributes),axis=0)\n",
    "# dataset_train5.instance_weights = np.concatenate((Z[4].instance_weights,Z[0].instance_weights,Z[1].instance_weights,Z[2].instance_weights),axis=0)\n",
    "# dataset_train5.instance_names = np.concatenate((Z[4].instance_names,Z[0].instance_names,Z[1].instance_names,Z[2].instance_names),axis=0)\n",
    "# dataset_train5.metadata = Z[4].metadata.copy()\n",
    "# dataset_test5= Z[3].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train1.convert_to_dataframe()[0].to_csv(\"Huangrui/german/german_train1.csv\",index=False)\n",
    "# dataset_test1.convert_to_dataframe()[0].to_csv(\"Huangrui/german/german_test1.csv\",index=False)\n",
    "# dataset_train2.convert_to_dataframe()[0].to_csv(\"Huangrui/german/german_train2.csv\",index=False)\n",
    "# dataset_test2.convert_to_dataframe()[0].to_csv(\"Huangrui/german/german_test2.csv\",index=False)\n",
    "# dataset_train3.convert_to_dataframe()[0].to_csv(\"Huangrui/german/german_train3.csv\",index=False)\n",
    "# dataset_test3.convert_to_dataframe()[0].to_csv(\"Huangrui/german/german_test3.csv\",index=False)\n",
    "# dataset_train4.convert_to_dataframe()[0].to_csv(\"Huangrui/german/german_train4.csv\",index=False)\n",
    "# dataset_test4.convert_to_dataframe()[0].to_csv(\"Huangrui/german/german_test4.csv\",index=False)\n",
    "# dataset_train5.convert_to_dataframe()[0].to_csv(\"Huangrui/german/german_train5.csv\",index=False)\n",
    "# dataset_test5.convert_to_dataframe()[0].to_csv(\"Huangrui/german/german_test5.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from sklearn.linear_model import Lasso\n",
    "# import pandas as pd\n",
    "# # 用部分 data训练 biased model\n",
    "# selected_dataset,_ = dataset_train5.split([0.6], shuffle=True)\n",
    "# _ =  dataset_test5\n",
    "# # Logistic regression classifier and predictions\n",
    "# scale_orig = StandardScaler()\n",
    "# X_train = scale_orig.fit_transform(selected_dataset.features)\n",
    "# y_train = selected_dataset.labels.ravel()\n",
    "# X_test = scale_orig.fit_transform(_.features)\n",
    "# y_test = _.labels.ravel()\n",
    "# lmod = Lasso(alpha = 0.001)\n",
    "# lmod.fit(X_train, y_train)\n",
    "# y_train_pred = lmod.predict(X_train)>0.5\n",
    "# y_test_pred = lmod.predict(X_test)>0.5\n",
    "# dataset_orig_train_pred = selected_dataset.copy(deepcopy=True)\n",
    "# dataset_orig_train_pred.labels = y_train_pred\n",
    "# dataset_orig_test_pred = _.copy(deepcopy=True)\n",
    "# dataset_orig_test_pred.labels = y_test_pred\n",
    "# print(np.mean(y_train_pred!=y_train))\n",
    "# print(np.mean(y_test_pred!=y_test))\n",
    "# metric_test = compute_metrics(_, dataset_orig_test_pred, \n",
    "#                 unprivileged_groups, privileged_groups)\n",
    "# metric_train = compute_metrics(selected_dataset, dataset_orig_train_pred,unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "# with open(\"Huangrui/german/german_model.pkl\", 'wb') as file:\n",
    "#     pickle.dump(lmod, file)\n",
    "# pickle.dump(lmod, open(\"Huangrui/german/german_model.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "budget = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train= GermanDataset(path=\"./Huangrui/german/german_train{}.csv\".format(K))\n",
    "dataset_orig_test= GermanDataset(path=\"./Huangrui/german/german_test{}.csv\".format(K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only use the budget% of the training data\n",
    "dataset_orig_train,_ = dataset_orig_train.split([budget], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up training data and display properties of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 51)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sex', 'age']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.]), array([1.])] [array([0.]), array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['month', 'credit_amount', 'investment_as_income_percentage', 'residence_since', 'age', 'number_of_credits', 'people_liable_for', 'sex', 'status=200+', 'status=<200', 'status=None', 'credit_history=Delay', 'credit_history=None/Paid', 'credit_history=Other', 'purpose=A40', 'purpose=A41', 'purpose=A410', 'purpose=A42', 'purpose=A43', 'purpose=A44', 'purpose=A45', 'purpose=A46', 'purpose=A48', 'purpose=A49', 'savings=500+', 'savings=<500', 'savings=Unknown/None', 'employment=1-4 years', 'employment=4+ years', 'employment=Unemployed', 'other_debtors=A101', 'other_debtors=A102', 'other_debtors=A103', 'property=A121', 'property=A122', 'property=A123', 'property=A124', 'installment_plans=A141', 'installment_plans=A142', 'installment_plans=A143', 'housing=A151', 'housing=A152', 'housing=A153', 'skill_level=A171', 'skill_level=A172', 'skill_level=A173', 'skill_level=A174', 'telephone=A191', 'telephone=A192', 'foreign_worker=A201', 'foreign_worker=A202']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig_train.privileged_protected_attributes, \n",
    "      dataset_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric for original training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.166667\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression classifier and predictions\n",
    "scale_orig = StandardScaler()\n",
    "X_train = scale_orig.fit_transform(dataset_orig_train.features)\n",
    "y_train = dataset_orig_train.labels.ravel()\n",
    "if protected_attribute_used == 1:\n",
    "    lmod = pickle.load(open('experiments/german'+str(K)+'_sex_bmodel.pkl','rb'))\n",
    "else:\n",
    "    lmod = pickle.load(open('experiments/german'+str(K)+'_age_bmodel.pkl','rb'))\n",
    "y_train_pred = lmod.predict(X_train)\n",
    "\n",
    "dataset_orig_train_pred = dataset_orig_train.copy(deepcopy=True)\n",
    "dataset_orig_train_pred.labels = y_train_pred.reshape(-1,1)\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(0.5-x))\n",
    "dataset_orig_train_pred.scores = sigmoid(y_train_pred).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain scores for validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "X_test = scale_orig.transform(dataset_orig_test_pred.features)\n",
    "y_test = dataset_orig_test_pred.labels\n",
    "dataset_orig_test_pred.scores = sigmoid(lmod.predict(X_test)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the optimal parameters from the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best threshold for classification only (no fairness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best balanced accuracy (no fairness constraints) = 0.8000\n",
      "Optimal classification threshold (no fairness constraints) = 0.5346\n"
     ]
    }
   ],
   "source": [
    "num_thresh = 100\n",
    "ba_arr = np.zeros(num_thresh)\n",
    "class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "    \n",
    "    fav_inds = dataset_orig_train_pred.scores > class_thresh\n",
    "    dataset_orig_train_pred.labels[fav_inds] = dataset_orig_train_pred.favorable_label\n",
    "    dataset_orig_train_pred.labels[~fav_inds] = dataset_orig_train_pred.unfavorable_label\n",
    "    \n",
    "    classified_metric_orig_train = ClassificationMetric(dataset_orig_train,\n",
    "                                             dataset_orig_train_pred, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "    \n",
    "    ba_arr[idx] = 0.5*(classified_metric_orig_train.true_positive_rate()\\\n",
    "                       +classified_metric_orig_train.true_negative_rate())\n",
    "\n",
    "best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "best_class_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "print(\"Best balanced accuracy (no fairness constraints) = %.4f\" % np.max(ba_arr))\n",
    "print(\"Optimal classification threshold (no fairness constraints) = %.4f\" % best_class_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate optimal parameters for the ROC method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                 privileged_groups=privileged_groups, \n",
    "                                 low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                  num_class_thresh=100, num_ROC_margin=50,\n",
    "                                  metric_name=metric_name,\n",
    "                                  metric_ub=metric_ub, metric_lb=metric_lb)\n",
    "ROC = ROC.fit(dataset_orig_train, dataset_orig_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal classification threshold (with fairness constraints) = 0.2773\n",
      "Optimal ROC margin = 0.2546\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC.classification_threshold)\n",
    "print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions from Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### train set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Raw predictions - No fairness constraints, only maximizing balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.8000\n",
      "Statistical parity difference = -0.5000\n",
      "Disparate impact = 0.0000\n",
      "Average odds difference = -0.3750\n",
      "Equal opportunity difference = -0.7500\n",
      "Theil index = 0.2877\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the test set\n",
    "fav_inds = dataset_orig_train_pred.scores > best_class_thresh\n",
    "dataset_orig_train_pred.labels[fav_inds] = dataset_orig_train_pred.favorable_label\n",
    "dataset_orig_train_pred.labels[~fav_inds] = dataset_orig_train_pred.unfavorable_label\n",
    "\n",
    "display(Markdown(\"#### train set\"))\n",
    "display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "metric_train_bef = compute_metrics(dataset_orig_train, dataset_orig_train_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### train set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Transformed predictions - With fairness constraints"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.8333\n",
      "Statistical parity difference = 0.3333\n",
      "Disparate impact = 1.5000\n",
      "Average odds difference = 0.5000\n",
      "Equal opportunity difference = 0.0000\n",
      "Theil index = 0.0362\n"
     ]
    }
   ],
   "source": [
    "# Transform the validation set\n",
    "dataset_transf_train_pred = ROC.predict(dataset_orig_train_pred)\n",
    "\n",
    "display(Markdown(\"#### train set\"))\n",
    "display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "metric_train_aft = compute_metrics(dataset_orig_train, dataset_transf_train_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing: Check if the metric optimized has not become worse\n",
    "assert np.abs(metric_train_aft[metric_name]) <= np.abs(metric_train_bef[metric_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions from Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Raw predictions - No fairness constraints, only maximizing balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.6710\n",
      "Statistical parity difference = -0.1500\n",
      "Disparate impact = 0.5714\n",
      "Average odds difference = -0.0859\n",
      "Equal opportunity difference = -0.1180\n",
      "Theil index = 0.4948\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the test set\n",
    "fav_inds = dataset_orig_test_pred.scores > best_class_thresh\n",
    "dataset_orig_test_pred.labels[fav_inds] = dataset_orig_test_pred.favorable_label\n",
    "dataset_orig_test_pred.labels[~fav_inds] = dataset_orig_test_pred.unfavorable_label\n",
    "\n",
    "display(Markdown(\"#### Test set\"))\n",
    "display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "metric_test_bef = compute_metrics(dataset_orig_test, dataset_orig_test_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Transformed predictions - With fairness constraints"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.6018\n",
      "Statistical parity difference = 0.6250\n",
      "Disparate impact = 2.6667\n",
      "Average odds difference = 0.7034\n",
      "Equal opportunity difference = 0.5133\n",
      "Theil index = 0.3868\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the transformed test set\n",
    "dataset_transf_test_pred = ROC.predict(dataset_orig_test_pred)\n",
    "\n",
    "display(Markdown(\"#### Test set\"))\n",
    "display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "metric_test_aft = compute_metrics(dataset_orig_test, dataset_transf_test_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Error for the test dataset is 0.41\n",
      "The Equal opportunity difference for the test dataset is 0.5133\n"
     ]
    }
   ],
   "source": [
    "#自己计算error, 不是balanced accuracy！！！\n",
    "metric_test_aft[\"Equal opportunity difference\"]\n",
    "print(\"The Error for the test dataset is {:.4}\".format(np.mean(dataset_orig_test.labels!=dataset_transf_test_pred.labels)))\n",
    "print(\"The Equal opportunity difference for the test dataset is {:.4}\".format(metric_test_aft[\"Equal opportunity difference\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
