{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook demonstrates the use of the Reject Option Classification (ROC) post-processing algorithm for bias mitigation.\n",
    "- The debiasing function used is implemented in the `RejectOptionClassification` class.\n",
    "- Divide the dataset into training, validation, and testing partitions.\n",
    "- Train classifier on original training data.\n",
    "- Estimate the optimal classification threshold, that maximizes balanced accuracy without fairness constraints.\n",
    "- Estimate the optimal classification threshold, and the critical region boundary (ROC margin) using a validation set for the desired constraint on fairness. The best parameters are those that maximize the classification threshold while satisfying the fairness constraints.\n",
    "- The constraints can be used on the following fairness measures:\n",
    "    * Statistical parity difference on the predictions of the classifier\n",
    "    * Average odds difference for the classifier\n",
    "    * Equal opportunity difference for the classifier\n",
    "- Determine the prediction scores for testing data. Using the estimated optimal classification threshold, compute accuracy and fairness metrics.\n",
    "- Using the determined optimal classification threshold and the ROC margin, adjust the predictions. Report accuracy and fairness metric on the new predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from warnings import warn\n",
    "\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import GermanDataset, CompasDataset\n",
    "from aif360.metrics import ClassificationMetric, BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.postprocessing.reject_option_classification\\\n",
    "        import RejectOptionClassification\n",
    "from common_utils import compute_metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive, FloatSlider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huangrui's Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preproc_data_german(protected_attributes=None):\n",
    "    \"\"\"\n",
    "    Load and pre-process german credit dataset.\n",
    "    Args:\n",
    "        protected_attributes(list or None): If None use all possible protected\n",
    "            attributes, else subset the protected attributes to the list.\n",
    "\n",
    "    Returns:\n",
    "        GermanDataset: An instance of GermanDataset with required pre-processing.\n",
    "\n",
    "    \"\"\"\n",
    "    def custom_preprocessing(df):\n",
    "        \"\"\" Custom pre-processing for German Credit Data\n",
    "        \"\"\"\n",
    "\n",
    "        def group_credit_hist(x):\n",
    "            if x in ['A30', 'A31', 'A32']:\n",
    "                return 'None/Paid'\n",
    "            elif x == 'A33':\n",
    "                return 'Delay'\n",
    "            elif x == 'A34':\n",
    "                return 'Other'\n",
    "            else:\n",
    "                return 'NA'\n",
    "\n",
    "        def group_employ(x):\n",
    "            if x == 'A71':\n",
    "                return 'Unemployed'\n",
    "            elif x in ['A72', 'A73']:\n",
    "                return '1-4 years'\n",
    "            elif x in ['A74', 'A75']:\n",
    "                return '4+ years'\n",
    "            else:\n",
    "                return 'NA'\n",
    "\n",
    "        def group_savings(x):\n",
    "            if x in ['A61', 'A62']:\n",
    "                return '<500'\n",
    "            elif x in ['A63', 'A64']:\n",
    "                return '500+'\n",
    "            elif x == 'A65':\n",
    "                return 'Unknown/None'\n",
    "            else:\n",
    "                return 'NA'\n",
    "\n",
    "        def group_status(x):\n",
    "            if x in ['A11', 'A12']:\n",
    "                return '<200'\n",
    "            elif x in ['A13']:\n",
    "                return '200+'\n",
    "            elif x == 'A14':\n",
    "                return 'None'\n",
    "            else:\n",
    "                return 'NA'\n",
    "\n",
    "        status_map = {'A91': 1.0, 'A93': 1.0, 'A94': 1.0,\n",
    "                    'A92': 0.0, 'A95': 0.0}\n",
    "        df['sex'] = df['personal_status'].replace(status_map)\n",
    "        #drop personal_status,not leak sex information\n",
    "        df = df.drop('personal_status', axis=1)\n",
    "\n",
    "        # group credit history, savings, and employment\n",
    "        df['credit_history'] = df['credit_history'].apply(lambda x: group_credit_hist(x))\n",
    "        df['savings'] = df['savings'].apply(lambda x: group_savings(x))\n",
    "        df['employment'] = df['employment'].apply(lambda x: group_employ(x))\n",
    "        df['age'] = (df['age'] >= 26).astype(float)\n",
    "        df['status'] = df['status'].apply(lambda x: group_status(x))\n",
    "\n",
    "        return df\n",
    "\n",
    "    # Feature partitions\n",
    "    D_features = ['sex', 'age'] if protected_attributes is None else protected_attributes\n",
    "    Y_features = ['credit']\n",
    "\n",
    "    # privileged classes\n",
    "    all_privileged_classes = {\"sex\": [1.0],\n",
    "                              \"age\": [1.0]}\n",
    "\n",
    "    # protected attribute maps\n",
    "    all_protected_attribute_maps = {\"sex\": {1.0: 'Male', 0.0: 'Female'},\n",
    "                                    \"age\": {1.0: 'Old', 0.0: 'Young'}}\n",
    "\n",
    "    return GermanDataset(\n",
    "        label_name=Y_features[0],\n",
    "        favorable_classes=[1],\n",
    "        protected_attribute_names=D_features,\n",
    "        privileged_classes=[all_privileged_classes[x] for x in D_features],\n",
    "        instance_weights_name=None,\n",
    "        features_to_keep=[],\n",
    "        metadata={ 'label_maps': [{1.0: 'Good Credit', 2.0: 'Bad Credit'}],\n",
    "                   'protected_attribute_maps': [all_protected_attribute_maps[x]\n",
    "                                for x in D_features]},\n",
    "        custom_preprocessing=custom_preprocessing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and specify options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import dataset\n",
    "dataset_used = \"german\" # \"german\", \"german\", \"compas\"\n",
    "protected_attribute_used = 1 # 1, 2\n",
    "\n",
    "\n",
    "#     dataset_orig = GermanDataset()\n",
    "if protected_attribute_used == 1:\n",
    "    privileged_groups = [{'sex': 1}]\n",
    "    unprivileged_groups = [{'sex': 0}]\n",
    "    dataset_orig = load_preproc_data_german(['sex'])\n",
    "else:\n",
    "    privileged_groups = [{'age': 1}]\n",
    "    unprivileged_groups = [{'age': 0}]\n",
    "    dataset_orig = load_preproc_data_german(['age'])\n",
    "\n",
    "        \n",
    "# Metric used (should be one of allowed_metrics)\n",
    "metric_name = \"Equal opportunity difference\"\n",
    "\n",
    "# Upper and lower bound on the fairness metric used\n",
    "metric_ub = 0.05\n",
    "metric_lb = -0.05\n",
    "        \n",
    "        \n",
    "#random seed for calibrated equal odds prediction\n",
    "random_seed = 12345679\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Verify metric name\n",
    "allowed_metrics = [\"Statistical parity difference\",\n",
    "                   \"Average odds difference\",\n",
    "                   \"Equal opportunity difference\"]\n",
    "if metric_name not in allowed_metrics:\n",
    "    raise ValueError(\"Metric name should be one of allowed metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['month',\n",
       " 'credit_amount',\n",
       " 'investment_as_income_percentage',\n",
       " 'residence_since',\n",
       " 'age',\n",
       " 'number_of_credits',\n",
       " 'people_liable_for',\n",
       " 'sex',\n",
       " 'status=200+',\n",
       " 'status=<200',\n",
       " 'status=None',\n",
       " 'credit_history=Delay',\n",
       " 'credit_history=None/Paid',\n",
       " 'credit_history=Other',\n",
       " 'purpose=A40',\n",
       " 'purpose=A41',\n",
       " 'purpose=A410',\n",
       " 'purpose=A42',\n",
       " 'purpose=A43',\n",
       " 'purpose=A44',\n",
       " 'purpose=A45',\n",
       " 'purpose=A46',\n",
       " 'purpose=A48',\n",
       " 'purpose=A49',\n",
       " 'savings=500+',\n",
       " 'savings=<500',\n",
       " 'savings=Unknown/None',\n",
       " 'employment=1-4 years',\n",
       " 'employment=4+ years',\n",
       " 'employment=Unemployed',\n",
       " 'other_debtors=A101',\n",
       " 'other_debtors=A102',\n",
       " 'other_debtors=A103',\n",
       " 'property=A121',\n",
       " 'property=A122',\n",
       " 'property=A123',\n",
       " 'property=A124',\n",
       " 'installment_plans=A141',\n",
       " 'installment_plans=A142',\n",
       " 'installment_plans=A143',\n",
       " 'housing=A151',\n",
       " 'housing=A152',\n",
       " 'housing=A153',\n",
       " 'skill_level=A171',\n",
       " 'skill_level=A172',\n",
       " 'skill_level=A173',\n",
       " 'skill_level=A174',\n",
       " 'telephone=A191',\n",
       " 'telephone=A192',\n",
       " 'foreign_worker=A201',\n",
       " 'foreign_worker=A202']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_orig.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 fold cross validation\n",
    "Z =  dataset_orig.split(5, shuffle=True,seed = random_seed)\n",
    "# i th fold\n",
    "dataset_train1 = Z[0].copy()\n",
    "dataset_train1.features = np.concatenate((Z[0].features,Z[1].features,Z[2].features,Z[3].features),axis=0)\n",
    "dataset_train1.scores = np.concatenate((Z[0].scores,Z[1].scores,Z[2].scores,Z[3].scores),axis=0)\n",
    "dataset_train1.labels = np.concatenate((Z[0].labels,Z[1].labels,Z[2].labels,Z[3].labels),axis=0)\n",
    "dataset_train1.protected_attributes = np.concatenate((Z[0].protected_attributes,Z[1].protected_attributes,Z[2].protected_attributes,Z[3].protected_attributes),axis=0)\n",
    "dataset_train1.instance_weights = np.concatenate((Z[0].instance_weights,Z[1].instance_weights,Z[2].instance_weights,Z[3].instance_weights),axis=0)\n",
    "dataset_train1.instance_names = np.concatenate((Z[0].instance_names,Z[1].instance_names,Z[2].instance_names,Z[3].instance_names),axis=0)\n",
    "dataset_train1.metadata = Z[0].metadata.copy()\n",
    "dataset_test1= Z[4].copy()\n",
    "\n",
    "dataset_train2 = Z[1].copy()\n",
    "dataset_train2.features = np.concatenate((Z[1].features,Z[2].features,Z[3].features,Z[4].features),axis=0)\n",
    "dataset_train2.scores = np.concatenate((Z[1].scores,Z[2].scores,Z[3].scores,Z[4].scores),axis=0)\n",
    "dataset_train2.labels = np.concatenate((Z[1].labels,Z[2].labels,Z[3].labels,Z[4].labels),axis=0)\n",
    "dataset_train2.protected_attributes = np.concatenate((Z[1].protected_attributes,Z[2].protected_attributes,Z[3].protected_attributes,Z[4].protected_attributes),axis=0)\n",
    "dataset_train2.instance_weights = np.concatenate((Z[1].instance_weights,Z[2].instance_weights,Z[3].instance_weights,Z[4].instance_weights),axis=0)\n",
    "dataset_train2.instance_names = np.concatenate((Z[1].instance_names,Z[2].instance_names,Z[3].instance_names,Z[4].instance_names),axis=0)\n",
    "dataset_train2.metadata = Z[1].metadata.copy()\n",
    "dataset_test2= Z[0].copy()\n",
    "\n",
    "dataset_train3 = Z[2].copy()\n",
    "dataset_train3.features = np.concatenate((Z[2].features,Z[3].features,Z[4].features,Z[0].features),axis=0)\n",
    "dataset_train3.scores = np.concatenate((Z[2].scores,Z[3].scores,Z[4].scores,Z[0].scores),axis=0)\n",
    "dataset_train3.labels = np.concatenate((Z[2].labels,Z[3].labels,Z[4].labels,Z[0].labels),axis=0)\n",
    "dataset_train3.protected_attributes = np.concatenate((Z[2].protected_attributes,Z[3].protected_attributes,Z[4].protected_attributes,Z[0].protected_attributes),axis=0)\n",
    "dataset_train3.instance_weights = np.concatenate((Z[2].instance_weights,Z[3].instance_weights,Z[4].instance_weights,Z[0].instance_weights),axis=0)\n",
    "dataset_train3.instance_names = np.concatenate((Z[2].instance_names,Z[3].instance_names,Z[4].instance_names,Z[0].instance_names),axis=0)\n",
    "dataset_train3.metadata = Z[2].metadata.copy()\n",
    "dataset_test3= Z[1].copy()\n",
    "\n",
    "dataset_train4 = Z[3].copy()\n",
    "dataset_train4.features = np.concatenate((Z[3].features,Z[4].features,Z[0].features,Z[1].features),axis=0)\n",
    "dataset_train4.scores = np.concatenate((Z[3].scores,Z[4].scores,Z[0].scores,Z[1].scores),axis=0)\n",
    "dataset_train4.labels = np.concatenate((Z[3].labels,Z[4].labels,Z[0].labels,Z[1].labels),axis=0)\n",
    "dataset_train4.protected_attributes = np.concatenate((Z[3].protected_attributes,Z[4].protected_attributes,Z[0].protected_attributes,Z[1].protected_attributes),axis=0)\n",
    "dataset_train4.instance_weights = np.concatenate((Z[3].instance_weights,Z[4].instance_weights,Z[0].instance_weights,Z[1].instance_weights),axis=0)\n",
    "dataset_train4.instance_names = np.concatenate((Z[3].instance_names,Z[4].instance_names,Z[0].instance_names,Z[1].instance_names),axis=0)\n",
    "dataset_train4.metadata = Z[3].metadata.copy()\n",
    "dataset_test4= Z[2].copy()\n",
    "\n",
    "dataset_train5 = Z[4].copy()\n",
    "dataset_train5.features = np.concatenate((Z[4].features,Z[0].features,Z[1].features,Z[2].features),axis=0)\n",
    "dataset_train5.scores = np.concatenate((Z[4].scores,Z[0].scores,Z[1].scores,Z[2].scores),axis=0)\n",
    "dataset_train5.labels = np.concatenate((Z[4].labels,Z[0].labels,Z[1].labels,Z[2].labels),axis=0)\n",
    "dataset_train5.protected_attributes = np.concatenate((Z[4].protected_attributes,Z[0].protected_attributes,Z[1].protected_attributes,Z[2].protected_attributes),axis=0)\n",
    "dataset_train5.instance_weights = np.concatenate((Z[4].instance_weights,Z[0].instance_weights,Z[1].instance_weights,Z[2].instance_weights),axis=0)\n",
    "dataset_train5.instance_names = np.concatenate((Z[4].instance_names,Z[0].instance_names,Z[1].instance_names,Z[2].instance_names),axis=0)\n",
    "dataset_train5.metadata = Z[4].metadata.copy()\n",
    "dataset_test5= Z[3].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_name = \"sex\"\n",
    "dataset_train1.convert_to_dataframe()[0].to_csv(\"Huangrui/german/{}/german_train1.csv\".format(protected_name),index=False)\n",
    "dataset_test1.convert_to_dataframe()[0].to_csv(\"Huangrui/german/{}/german_test1.csv\".format(protected_name),index=False)\n",
    "dataset_train2.convert_to_dataframe()[0].to_csv(\"Huangrui/german/{}/german_train2.csv\".format(protected_name),index=False)\n",
    "dataset_test2.convert_to_dataframe()[0].to_csv(\"Huangrui/german/{}/german_test2.csv\".format(protected_name),index=False)\n",
    "dataset_train3.convert_to_dataframe()[0].to_csv(\"Huangrui/german/{}/german_train3.csv\".format(protected_name),index=False)\n",
    "dataset_test3.convert_to_dataframe()[0].to_csv(\"Huangrui/german/{}/german_test3.csv\".format(protected_name),index=False)\n",
    "dataset_train4.convert_to_dataframe()[0].to_csv(\"Huangrui/german/{}/german_train4.csv\".format(protected_name),index=False)\n",
    "dataset_test4.convert_to_dataframe()[0].to_csv(\"Huangrui/german/{}/german_test4.csv\".format(protected_name),index=False)\n",
    "dataset_train5.convert_to_dataframe()[0].to_csv(\"Huangrui/german/{}/german_train5.csv\".format(protected_name),index=False)\n",
    "dataset_test5.convert_to_dataframe()[0].to_csv(\"Huangrui/german/{}/german_test5.csv\".format(protected_name),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train_valid= dataset_train5.copy()\n",
    "dataset_orig_test= dataset_test5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset and split into train and test\n",
    "dataset_orig_train, dataset_orig_valid = dataset_orig_train_valid.split([0.8], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up training data and display properties of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 51)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 2.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sex']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.])] [array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['month', 'credit_amount', 'investment_as_income_percentage', 'residence_since', 'age', 'number_of_credits', 'people_liable_for', 'sex', 'status=200+', 'status=<200', 'status=None', 'credit_history=Delay', 'credit_history=None/Paid', 'credit_history=Other', 'purpose=A40', 'purpose=A41', 'purpose=A410', 'purpose=A42', 'purpose=A43', 'purpose=A44', 'purpose=A45', 'purpose=A46', 'purpose=A48', 'purpose=A49', 'savings=500+', 'savings=<500', 'savings=Unknown/None', 'employment=1-4 years', 'employment=4+ years', 'employment=Unemployed', 'other_debtors=A101', 'other_debtors=A102', 'other_debtors=A103', 'property=A121', 'property=A122', 'property=A123', 'property=A124', 'installment_plans=A141', 'installment_plans=A142', 'installment_plans=A143', 'housing=A151', 'housing=A152', 'housing=A153', 'skill_level=A171', 'skill_level=A172', 'skill_level=A173', 'skill_level=A174', 'telephone=A191', 'telephone=A192', 'foreign_worker=A201', 'foreign_worker=A202']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig_train.privileged_protected_attributes, \n",
    "      dataset_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric for original training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.065478\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression classifier and predictions\n",
    "scale_orig = StandardScaler()\n",
    "X_train = scale_orig.fit_transform(dataset_orig_train.features)\n",
    "y_train = dataset_orig_train.labels.ravel()\n",
    "\n",
    "lmod = LogisticRegression()\n",
    "lmod.fit(X_train, y_train)\n",
    "y_train_pred = lmod.predict(X_train)\n",
    "\n",
    "# positive class index\n",
    "pos_ind = np.where(lmod.classes_ == dataset_orig_train.favorable_label)[0][0]\n",
    "\n",
    "dataset_orig_train_pred = dataset_orig_train.copy(deepcopy=True)\n",
    "dataset_orig_train_pred.labels = y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain scores for validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_valid_pred = dataset_orig_valid.copy(deepcopy=True)\n",
    "X_valid = scale_orig.transform(dataset_orig_valid_pred.features)\n",
    "y_valid = dataset_orig_valid_pred.labels\n",
    "dataset_orig_valid_pred.scores = lmod.predict_proba(X_valid)[:,pos_ind].reshape(-1,1)\n",
    "\n",
    "dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "X_test = scale_orig.transform(dataset_orig_test_pred.features)\n",
    "y_test = dataset_orig_test_pred.labels\n",
    "dataset_orig_test_pred.scores = lmod.predict_proba(X_test)[:,pos_ind].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the optimal parameters from the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best threshold for classification only (no fairness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best balanced accuracy (no fairness constraints) = 0.7044\n",
      "Optimal classification threshold (no fairness constraints) = 0.6831\n"
     ]
    }
   ],
   "source": [
    "num_thresh = 100\n",
    "ba_arr = np.zeros(num_thresh)\n",
    "class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "    \n",
    "    fav_inds = dataset_orig_valid_pred.scores > class_thresh\n",
    "    dataset_orig_valid_pred.labels[fav_inds] = dataset_orig_valid_pred.favorable_label\n",
    "    dataset_orig_valid_pred.labels[~fav_inds] = dataset_orig_valid_pred.unfavorable_label\n",
    "    \n",
    "    classified_metric_orig_valid = ClassificationMetric(dataset_orig_valid,\n",
    "                                             dataset_orig_valid_pred, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "    \n",
    "    ba_arr[idx] = 0.5*(classified_metric_orig_valid.true_positive_rate()\\\n",
    "                       +classified_metric_orig_valid.true_negative_rate())\n",
    "\n",
    "best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "best_class_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "print(\"Best balanced accuracy (no fairness constraints) = %.4f\" % np.max(ba_arr))\n",
    "print(\"Optimal classification threshold (no fairness constraints) = %.4f\" % best_class_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate optimal parameters for the ROC method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                 privileged_groups=privileged_groups, \n",
    "                                 low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                  num_class_thresh=100, num_ROC_margin=50,\n",
    "                                  metric_name=metric_name,\n",
    "                                  metric_ub=metric_ub, metric_lb=metric_lb)\n",
    "ROC = ROC.fit(dataset_orig_valid, dataset_orig_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal classification threshold (with fairness constraints) = 0.7524\n",
      "Optimal ROC margin = 0.0808\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC.classification_threshold)\n",
    "print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions from Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Validation set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Raw predictions - No fairness constraints, only maximizing balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.7044\n",
      "Statistical parity difference = -0.1980\n",
      "Disparate impact = 0.6710\n",
      "Average odds difference = -0.1941\n",
      "Equal opportunity difference = -0.1405\n",
      "Theil index = 0.2803\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the test set\n",
    "fav_inds = dataset_orig_valid_pred.scores > best_class_thresh\n",
    "dataset_orig_valid_pred.labels[fav_inds] = dataset_orig_valid_pred.favorable_label\n",
    "dataset_orig_valid_pred.labels[~fav_inds] = dataset_orig_valid_pred.unfavorable_label\n",
    "\n",
    "display(Markdown(\"#### Validation set\"))\n",
    "display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "metric_valid_bef = compute_metrics(dataset_orig_valid, dataset_orig_valid_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Validation set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Transformed predictions - With fairness constraints"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.7143\n",
      "Statistical parity difference = -0.0221\n",
      "Disparate impact = 0.9482\n",
      "Average odds difference = -0.0100\n",
      "Equal opportunity difference = 0.0217\n",
      "Theil index = 0.3685\n"
     ]
    }
   ],
   "source": [
    "# Transform the validation set\n",
    "dataset_transf_valid_pred = ROC.predict(dataset_orig_valid_pred)\n",
    "\n",
    "display(Markdown(\"#### Validation set\"))\n",
    "display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "metric_valid_aft = compute_metrics(dataset_orig_valid, dataset_transf_valid_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing: Check if the metric optimized has not become worse\n",
    "assert np.abs(metric_valid_aft[metric_name]) <= np.abs(metric_valid_bef[metric_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions from Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Raw predictions - No fairness constraints, only maximizing balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.7516\n",
      "Statistical parity difference = -0.2016\n",
      "Disparate impact = 0.7126\n",
      "Average odds difference = -0.1672\n",
      "Equal opportunity difference = -0.1122\n",
      "Theil index = 0.1839\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the test set\n",
    "fav_inds = dataset_orig_test_pred.scores > best_class_thresh\n",
    "dataset_orig_test_pred.labels[fav_inds] = dataset_orig_test_pred.favorable_label\n",
    "dataset_orig_test_pred.labels[~fav_inds] = dataset_orig_test_pred.unfavorable_label\n",
    "\n",
    "display(Markdown(\"#### Test set\"))\n",
    "display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "metric_test_bef = compute_metrics(dataset_orig_test, dataset_orig_test_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Transformed predictions - With fairness constraints"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.7376\n",
      "Statistical parity difference = 0.0454\n",
      "Disparate impact = 1.0971\n",
      "Average odds difference = 0.0881\n",
      "Equal opportunity difference = 0.1151\n",
      "Theil index = 0.2981\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the transformed test set\n",
    "dataset_transf_test_pred = ROC.predict(dataset_orig_test_pred)\n",
    "\n",
    "display(Markdown(\"#### Test set\"))\n",
    "display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "metric_test_aft = compute_metrics(dataset_orig_test, dataset_transf_test_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Error for the test dataset is 0.2624\n",
      "The Equal opportunity difference for the test dataset is 0.1151\n"
     ]
    }
   ],
   "source": [
    "metric_test_aft[\"Balanced accuracy\"]\n",
    "metric_test_aft[\"Equal opportunity difference\"]\n",
    "print(\"The Error for the test dataset is {:.4}\".format(1-metric_test_aft[\"Balanced accuracy\"]))\n",
    "print(\"The Equal opportunity difference for the test dataset is {:.4}\".format(metric_test_aft[\"Equal opportunity difference\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
