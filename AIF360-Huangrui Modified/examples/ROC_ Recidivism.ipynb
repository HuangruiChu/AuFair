{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook demonstrates the use of the Reject Option Classification (ROC) post-processing algorithm for bias mitigation.\n",
    "- The debiasing function used is implemented in the `RejectOptionClassification` class.\n",
    "- Divide the dataset into training, validation, and testing partitions.\n",
    "- Train classifier on original training data.\n",
    "- Estimate the optimal classification threshold, that maximizes balanced accuracy without fairness constraints.\n",
    "- Estimate the optimal classification threshold, and the critical region boundary (ROC margin) using a validation set for the desired constraint on fairness. The best parameters are those that maximize the classification threshold while satisfying the fairness constraints.\n",
    "- The constraints can be used on the following fairness measures:\n",
    "    * Statistical parity difference on the predictions of the classifier\n",
    "    * Average odds difference for the classifier\n",
    "    * Equal opportunity difference for the classifier\n",
    "- Determine the prediction scores for testing data. Using the estimated optimal classification threshold, compute accuracy and fairness metrics.\n",
    "- Using the determined optimal classification threshold and the ROC margin, adjust the predictions. Report accuracy and fairness metric on the new predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\n",
      "`load_boston` has been removed from scikit-learn since version 1.2.\n",
      "\n",
      "The Boston housing prices dataset has an ethical problem: as\n",
      "investigated in [1], the authors of this dataset engineered a\n",
      "non-invertible variable \"B\" assuming that racial self-segregation had a\n",
      "positive impact on house prices [2]. Furthermore the goal of the\n",
      "research that led to the creation of this dataset was to study the\n",
      "impact of air quality but it did not give adequate demonstration of the\n",
      "validity of this assumption.\n",
      "\n",
      "The scikit-learn maintainers therefore strongly discourage the use of\n",
      "this dataset unless the purpose of the code is to study and educate\n",
      "about ethical issues in data science and machine learning.\n",
      "\n",
      "In this special case, you can fetch the dataset from the original\n",
      "source::\n",
      "\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "    target = raw_df.values[1::2, 2]\n",
      "\n",
      "Alternative datasets include the California housing dataset and the\n",
      "Ames housing dataset. You can load the datasets as follows::\n",
      "\n",
      "    from sklearn.datasets import fetch_california_housing\n",
      "    housing = fetch_california_housing()\n",
      "\n",
      "for the California housing dataset and::\n",
      "\n",
      "    from sklearn.datasets import fetch_openml\n",
      "    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "for the Ames housing dataset.\n",
      "\n",
      "[1] M Carlisle.\n",
      "\"Racist data destruction?\"\n",
      "<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n",
      "\n",
      "[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n",
      "\"Hedonic housing prices and the demand for clean air.\"\n",
      "Journal of environmental economics and management 5.1 (1978): 81-102.\n",
      "<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
      ": LawSchoolGPADataset will be unavailable. To install, run:\n",
      "pip install 'aif360[LawSchoolGPA]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from warnings import warn\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import ClassificationMetric, BinaryLabelDatasetMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "        import load_preproc_data_adult, load_preproc_data_german, load_preproc_data_compas\n",
    "from aif360.algorithms.postprocessing.reject_option_classification\\\n",
    "        import RejectOptionClassification\n",
    "from common_utils import compute_metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive, FloatSlider\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huangrui's Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_mappings = {\n",
    "    'label_maps': [{1.0: 'Did recid.', 0.0: 'No recid.'}],\n",
    "    'protected_attribute_maps': [{0.0: 'Male', 1.0: 'Female'},\n",
    "                                 {1.0: 'Caucasian', 0.0: 'Not Caucasian'}]\n",
    "}\n",
    "def code_continuous(df,collist,Nlevel):\n",
    "    for col in collist:\n",
    "        for q in range(1,Nlevel,1):\n",
    "            threshold = df[~np.isnan(df[col])][col].quantile(float(q)/Nlevel)\n",
    "            df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
    "    df.drop(collist,axis = 1, inplace = True)\n",
    "class CompasDataset(StandardDataset):\n",
    "    \"\"\"ProPublica COMPAS Dataset.\n",
    "\n",
    "    See :file:`aif360/data/raw/compas/README.md`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, label_name='Y', favorable_classes=[1],\n",
    "                 protected_attribute_names=['sex'],\n",
    "                 privileged_classes=[[1]],\n",
    "                 instance_weights_name=None,\n",
    "                 categorical_features=[],\n",
    "                 features_to_keep=[],\n",
    "                 features_to_drop=[], na_values=[],\n",
    "                 custom_preprocessing=None,\n",
    "                 metadata=default_mappings,\n",
    "                 path='./Huangrui/recidivism/recidivism_test1.csv'):\n",
    "\n",
    "    \n",
    "        df = pd.read_csv(path,index_col=False)\n",
    "        df.rename(columns={'Probationerssex_Female': 'sex'}, inplace=True)\n",
    "        df.drop([\"Probationerssex_Male\",\"Probationerssex_Notascertained\"], axis=1, inplace=True)\n",
    "        numericals = [col for col in df.columns if len(df[col].unique())>2 and max(df[col])>1]\n",
    "        code_continuous(df,numericals, 5)\n",
    "        \n",
    "        super(CompasDataset, self).__init__(df=df, label_name=label_name,\n",
    "            favorable_classes=favorable_classes,\n",
    "            protected_attribute_names=protected_attribute_names,\n",
    "            privileged_classes=privileged_classes,\n",
    "            instance_weights_name=instance_weights_name,\n",
    "            categorical_features=categorical_features,\n",
    "            features_to_keep=features_to_keep,\n",
    "            features_to_drop=features_to_drop, na_values=na_values,\n",
    "            custom_preprocessing=custom_preprocessing, metadata=metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and specify options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import dataset\n",
    "dataset_used = \"compas\" # \"adult\", \"german\", \"compas\"\n",
    "\n",
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]\n",
    "        \n",
    "# Metric used (should be one of allowed_metrics)\n",
    "metric_name = \"Equal opportunity difference\"\n",
    "\n",
    "# Upper and lower bound on the fairness metric used\n",
    "metric_ub = 0.05\n",
    "metric_lb = -0.05\n",
    "        \n",
    "#random seed for calibrated equal odds prediction\n",
    "np.random.seed(1)\n",
    "\n",
    "# Verify metric name\n",
    "allowed_metrics = [\"Statistical parity difference\",\n",
    "                   \"Average odds difference\",\n",
    "                   \"Equal opportunity difference\"]\n",
    "if metric_name not in allowed_metrics:\n",
    "    raise ValueError(\"Metric name should be one of allowed metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=5\n",
    "budget = 1 #0.01, 0.1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n"
     ]
    }
   ],
   "source": [
    "dataset_orig_train= CompasDataset(path=\"./Huangrui/recidivism/recidivism_train{}.csv\".format(K),\n",
    "                                       protected_attribute_names=[\"sex\"],\n",
    "                                       privileged_classes=[[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
      "C:\\TEMP\\ipykernel_14808\\3106683521.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n"
     ]
    }
   ],
   "source": [
    "dataset_orig_test= CompasDataset(path=\"./Huangrui/recidivism/recidivism_test{}.csv\".format(K),\n",
    "                                       protected_attribute_names=[\"sex\"],\n",
    "                                       privileged_classes=[[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset and split into train and test\n",
    "if budget == 0.01:\n",
    "    #for budget = 0.01 use seed 43 \n",
    "    dataset_orig_train,_ = dataset_orig_train.split([budget ], shuffle=True)\n",
    "else:\n",
    "    dataset_orig_train,_ = dataset_orig_train.split([budget], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up training data and display properties of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9317, 528)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sex']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.])] [array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Threedigitoffensecode_Allotherexceptcannabis', 'Threedigitoffensecode_Armedrobbery', 'Threedigitoffensecode_Assault', 'Threedigitoffensecode_Attemptedhomicide', 'Threedigitoffensecode_Burglary', 'Threedigitoffensecode_Burglaryinvolvingvict', 'Threedigitoffensecode_Cannabis', 'Threedigitoffensecode_Drugpossession', 'Threedigitoffensecode_Drugtrafficking', 'Threedigitoffensecode_Escape', 'Threedigitoffensecode_ForcibleRape', 'Threedigitoffensecode_Forgery', 'Threedigitoffensecode_Fraud', 'Threedigitoffensecode_Homicides', 'Threedigitoffensecode_Manslaughter', 'Threedigitoffensecode_Miscellaneous', 'Threedigitoffensecode_Motorvehicularoffenses', 'Threedigitoffensecode_Murder', 'Threedigitoffensecode_Narcotics,cocaine', 'Threedigitoffensecode_Notascertained', 'Threedigitoffensecode_Otherstructure', 'Threedigitoffensecode_Possessionstolenproperty', 'Threedigitoffensecode_Rape', 'Threedigitoffensecode_Residential', 'Threedigitoffensecode_Robbery', 'Threedigitoffensecode_Sexrelatedoffenses', 'Threedigitoffensecode_Statutory', 'Threedigitoffensecode_Theft', 'Threedigitoffensecode_Unarmedrobbery', 'Threedigitoffensecode_Weaponsrelated', 'sex', 'Percentageoftimeemployed_40to60%', 'Percentageoftimeemployed_60%ormore', 'Percentageoftimeemployed_Notascertained', 'Percentageoftimeemployed_Under40%', 'Maritalstatus_Divorced/separated', 'Maritalstatus_Married/widower', 'Maritalstatus_Notascertained', 'Maritalstatus_Single', 'Educationallevelattained_Collegedegree', 'Educationallevelattained_Gradeschool', 'Educationallevelattained_Highschool(GED)', 'Educationallevelattained_Notascertained', 'Educationallevelattained_Somecollege', 'Educationallevelattained_Somehighschool', 'Addresschangesintwelvemonths_None', 'Addresschangesintwelvemonths_Notascertained', 'Addresschangesintwelvemonths_One', 'Addresschangesintwelvemonths_Twoormore', 'Drugabusehistory_Frequentabuse', 'Drugabusehistory_Nodrugabuseproblem', 'Drugabusehistory_Notascertained', 'Drugabusehistory_Occasionalabuse', 'Numberofpriorfelonyconvictions_None', 'Numberofpriorfelonyconvictions_Notascertained', 'Numberofpriorfelonyconvictions_One', 'Numberofpriorfelonyconvictions_Twoormore', 'Typeofsupervision_Notapplicable', 'Typeofsupervision_Notascertained', 'Typeofsupervision_Supervised', 'Typeofsupervision_Unsupervised', 'Transferredtoanotherjurisdiction_No', 'Transferredtoanotherjurisdiction_Notascertained', 'Transferredtoanotherjurisdiction_Yes,instate', 'Transferredtoanotherjurisdiction_Yes,outofstate', 'Reasonforbeingtransferred_Absconded', 'Reasonforbeingtransferred_Died', 'Reasonforbeingtransferred_Notascertained', 'Reasonforbeingtransferred_Other', 'Reasonforbeingtransferred_Prison', 'Reasonforbeingtransferred_Termserved', 'Probationrecordsreflectjailbeingimposed_No', 'Probationrecordsreflectjailbeingimposed_Notascertained', 'Probationrecordsreflectjailbeingimposed_Yes', 'Recodeofprobationersassigned_1to50', 'Recodeofprobationersassigned_101to150', 'Recodeofprobationersassigned_151to200', 'Recodeofprobationersassigned_201to250', 'Recodeofprobationersassigned_250ormore', 'Recodeofprobationersassigned_51to100', 'Recodeofprobationersassigned_Notascertained', 'Communityplacementordered_No', 'Communityplacementordered_Notascertained', 'Communityplacementordered_Yes', 'Statusofcompliancewithcommunityplacement_Notapplicable', 'Statusofcompliancewithcommunityplacement_Notascertained', 'Statusofcompliancewithcommunityplacement_Notsatisfied', 'Statusofcompliancewithcommunityplacement_Progressmade', 'Statusofcompliancewithcommunityplacement_Satisfied', 'Alcoholtreatmentordered_No', 'Alcoholtreatmentordered_Notascertained', 'Alcoholtreatmentordered_Yes', 'Statusofcompliancewithalcoholtreatment_Notapplicable', 'Statusofcompliancewithalcoholtreatment_Notascertained', 'Statusofcompliancewithalcoholtreatment_Notsatisfied', 'Statusofcompliancewithalcoholtreatment_Progressmade', 'Statusofcompliancewithalcoholtreatment_Satisfied', 'Drugtreatmentordered_No', 'Drugtreatmentordered_Notascertained', 'Drugtreatmentordered_Yes', 'Statusofcompliancewithdrugtreatment_Notapplicable', 'Statusofcompliancewithdrugtreatment_Notascertained', 'Statusofcompliancewithdrugtreatment_Notsatisfied', 'Statusofcompliancewithdrugtreatment_Progressmade', 'Statusofcompliancewithdrugtreatment_Satisfied', 'Drugtestingordered_No', 'Drugtestingordered_Notascertained', 'Drugtestingordered_Yes', 'Statusofcompliancewithdrugtesting_Notapplicable', 'Statusofcompliancewithdrugtesting_Notascertained', 'Statusofcompliancewithdrugtesting_Notsatisfied', 'Statusofcompliancewithdrugtesting_Progressmade', 'Statusofcompliancewithdrugtesting_Satisfied', 'Mentalhealthcounsellingordered_No', 'Mentalhealthcounsellingordered_Notascertained', 'Mentalhealthcounsellingordered_Yes', 'Statusofcompliancewithmentalhealthcounselling_Notapplicable', 'Statusofcompliancewithmentalhealthcounselling_Notascertained', 'Statusofcompliancewithmentalhealthcounselling_Notsatisfied', 'Statusofcompliancewithmentalhealthcounselling_Progressmade', 'Statusofcompliancewithmentalhealthcounselling_Satisfied', 'Housearrestordered_No', 'Housearrestordered_Notascertained', 'Housearrestordered_Yes', 'Statusofcompliancewithhousearrestorder_Notapplicable', 'Statusofcompliancewithhousearrestorder_Notascertained', 'Statusofcompliancewithhousearrestorder_Notsatisfied', 'Statusofcompliancewithhousearrestorder_Progressmade', 'Statusofcompliancewithhousearrestorder_Satisfied', 'Dayprogrammingordered_No', 'Dayprogrammingordered_Notascertained', 'Dayprogrammingordered_Yes', 'Statusofcompliancewithdayprogramming_Notapplicable', 'Statusofcompliancewithdayprogramming_Notascertained', 'Statusofcompliancewithdayprogramming_Notsatisfied', 'Statusofcompliancewithdayprogramming_Progressmade', 'Statusofcompliancewithdayprogramming_Satisfied', 'Communityserviceordered_No', 'Communityserviceordered_Notascertained', 'Communityserviceordered_Yes', 'Statusofcompliancewithcommunityserviceorder_Notapplicable', 'Statusofcompliancewithcommunityserviceorder_Notascertained', 'Statusofcompliancewithcommunityserviceorder_Notsatisfied', 'Statusofcompliancewithcommunityserviceorder_Progressmade', 'Statusofcompliancewithcommunityserviceorder_Satisfied', 'Recodefornumberofcommunityhours_1to49', 'Recodefornumberofcommunityhours_100to199', 'Recodefornumberofcommunityhours_200to499', 'Recodefornumberofcommunityhours_50to99', 'Recodefornumberofcommunityhours_500ormore', 'Recodefornumberofcommunityhours_Notapplicable', 'Recodeofpercentofcommunityhours_0%', 'Recodeofpercentofcommunityhours_1to24%', 'Recodeofpercentofcommunityhours_100%', 'Recodeofpercentofcommunityhours_25to49%', 'Recodeofpercentofcommunityhours_50to74%', 'Recodeofpercentofcommunityhours_75to99%', 'Recodeofpercentofcommunityhours_Notapplicable', 'Recodeofcompliancepercent_0%', 'Recodeofcompliancepercent_1to24%', 'Recodeofcompliancepercent_100%', 'Recodeofcompliancepercent_25to49%', 'Recodeofcompliancepercent_50to74%', 'Recodeofcompliancepercent_75to99%', 'Recodeofcompliancepercent_Notapplicable', 'Recodeofcompliancepercent_Notascertained', 'Howhousearrestmonitored_Electronically', 'Howhousearrestmonitored_Notapplicable', 'Howhousearrestmonitored_Notascertained', 'Howhousearrestmonitored_Other', 'Howhousearrestmonitored_Randomcalls', 'Probationflag_Personprobbefore1986', 'Probationflag_Persstartprobw/1986sent', 'Courtfineimposed_No', 'Courtfineimposed_Notascertained', 'Courtfineimposed_Yes,amountknown', 'Courtfineimposed_Yes,amountunknown', 'Courtfeesimposed_No', 'Courtfeesimposed_Notascertained', 'Courtfeesimposed_Yes,amountknown', 'Courtfeesimposed_Yes,amountunknown', 'Probationfeesimposed_No', 'Probationfeesimposed_Notascertained', 'Probationfeesimposed_Yes,amountknown', 'Probationfeesimposed_Yes,amountunknown', 'Restitutionimposed_No', 'Restitutionimposed_Notascertained', 'Restitutionimposed_Yes,amountknown', 'Restitutionimposed_Yes,amountunknown', 'Victimcompensationfeeimposed_No', 'Victimcompensationfeeimposed_Notascertained', 'Victimcompensationfeeimposed_Yes,amountknown', 'Victimcompensationfeeimposed_Yes,amountunknown', 'Otherfeesimposed_No', 'Otherfeesimposed_Notascertained', 'Otherfeesimposed_Yes,amountknown', 'Otherfeesimposed_Yes,amountunknown', 'Publicdefenderfeesimposed_No', 'Publicdefenderfeesimposed_Notascertained', 'Publicdefenderfeesimposed_Yes,amountknown', 'Publicdefenderfeesimposed_Yes,amountunknown', 'Financialflag_Atleast1fineassess', 'Financialflag_Nofineassessimposed', 'Financialflag_Notapplicable', 'Totalfeesassessed_No', 'Totalfeesassessed_Notascertained', 'Totalfeesassessed_Yes', 'Totalotherfees_No', 'Totalotherfees_Notascertained', 'Totalotherfees_Yes', 'Distributionofallcourtassessments_$100to$249', 'Distributionofallcourtassessments_$1000to$4999', 'Distributionofallcourtassessments_$250to$499', 'Distributionofallcourtassessments_$500to$999', 'Distributionofallcourtassessments_$5000ormore', 'Distributionofallcourtassessments_Lessthan$100', 'Distributionofallcourtassessments_Notascertained', 'Distributionofsupervisionfees_$100to$249', 'Distributionofsupervisionfees_$1000to$4999', 'Distributionofsupervisionfees_$250to$499', 'Distributionofsupervisionfees_$500to$999', 'Distributionofsupervisionfees_Lessthan$100', 'Distributionofsupervisionfees_Notascertained', 'Distributionofrestitutionfees_$100to$249', 'Distributionofrestitutionfees_$1000to$4999', 'Distributionofrestitutionfees_$250to$499', 'Distributionofrestitutionfees_$500to$999', 'Distributionofrestitutionfees_$5000ormore', 'Distributionofrestitutionfees_Lessthan$100', 'Distributionofrestitutionfees_Notascertained', 'Distributionofallotherfees_$100to$249', 'Distributionofallotherfees_$1000to$4999', 'Distributionofallotherfees_$250to$499', 'Distributionofallotherfees_$500to$999', 'Distributionofallotherfees_$5000ormore', 'Distributionofallotherfees_Lessthan$100', 'Distributionofallotherfees_Notascertained', 'Distributionofallotherassessments_$100to$249', 'Distributionofallotherassessments_$1000to$4999', 'Distributionofallotherassessments_$250to$499', 'Distributionofallotherassessments_$500to$999', 'Distributionofallotherassessments_$5000ormore', 'Distributionofallotherassessments_Lessthan$100', 'Distributionofallotherassessments_Notascertained', 'Distributionofpercentofassessments-court_0%', 'Distributionofpercentofassessments-court_1to24%', 'Distributionofpercentofassessments-court_100%', 'Distributionofpercentofassessments-court_25to49%', 'Distributionofpercentofassessments-court_50to74%', 'Distributionofpercentofassessments-court_75to99%', 'Distributionofpercentofassessments-court_Notascertained', 'Distributionofpercentofassessments-supervision_0%', 'Distributionofpercentofassessments-supervision_1to24%', 'Distributionofpercentofassessments-supervision_100%', 'Distributionofpercentofassessments-supervision_25to49%', 'Distributionofpercentofassessments-supervision_50to74%', 'Distributionofpercentofassessments-supervision_75to99%', 'Distributionofpercentofassessments-supervision_Notascertained', 'Distributionofpercentofassessments-restitution_0%', 'Distributionofpercentofassessments-restitution_1to24%', 'Distributionofpercentofassessments-restitution_100%', 'Distributionofpercentofassessments-restitution_25to49%', 'Distributionofpercentofassessments-restitution_50to74%', 'Distributionofpercentofassessments-restitution_75to99%', 'Distributionofpercentofassessments-restitution_Notascertained', 'Distributionofpercentofassessments-other_0%', 'Distributionofpercentofassessments-other_1to24%', 'Distributionofpercentofassessments-other_100%', 'Distributionofpercentofassessments-other_25to49%', 'Distributionofpercentofassessments-other_50to74%', 'Distributionofpercentofassessments-other_75to99%', 'Distributionofpercentofassessments-other_Notascertained', 'Distributionofpercentofassessments-total_0%', 'Distributionofpercentofassessments-total_1to24%', 'Distributionofpercentofassessments-total_100%', 'Distributionofpercentofassessments-total_25to49%', 'Distributionofpercentofassessments-total_50to74%', 'Distributionofpercentofassessments-total_75to99%', 'Distributionofpercentofassessments-total_Notascertained', 'Agedistribution_20to24years', 'Agedistribution_25to29years', 'Agedistribution_30to39years', 'Agedistribution_40to49years', 'Agedistribution_50orolder', 'Agedistribution_Notascertained', 'Agedistribution_Under20', 'Recodeoftimetofirsthearing_0to183days', 'Recodeoftimetofirsthearing_184to365days', 'Recodeoftimetofirsthearing_366to548days', 'Recodeoftimetofirsthearing_549ormoredays', 'Recodeoftimetofirsthearing_Notapplicable', 'Recodeoftimetofirsthearing_Notascertained', 'Recodeoftimefromviolationtohearing_0to14days', 'Recodeoftimefromviolationtohearing_15to31days', 'Recodeoftimefromviolationtohearing_32to46days', 'Recodeoftimefromviolationtohearing_47to62days', 'Recodeoftimefromviolationtohearing_63ormoredays', 'Recodeoftimefromviolationtohearing_Notapplicable', 'Recodeoftimefromviolationtohearing_Notascertained', 'Riskflag_No', 'Riskflag_Yes', 'Raceofprobationer_AmerIndian/AlaskanNative', 'Raceofprobationer_Asian/PacificIslander', 'Raceofprobationer_Black', 'Raceofprobationer_Notascertained', 'Raceofprobationer_Other', 'Raceofprobationer_White', 'Ethnicityofprobationer_Hispanic', 'Ethnicityofprobationer_Non-Hispanic', 'Ethnicityofprobationer_Notascertained', '1986convictionoffense_Aggravatedassault', '1986convictionoffense_Burglary', '1986convictionoffense_Drugtrafficking', '1986convictionoffense_Larceny/autotheft', '1986convictionoffense_Murder/nonnegmanslaughter', '1986convictionoffense_Otherfelony', '1986convictionoffense_Rape', '1986convictionoffense_Robbery', 'Jurisdiction_BaltimoreCity,MD', 'Jurisdiction_BaltimoreCounty,MD', 'Jurisdiction_BexarCounty,TX', 'Jurisdiction_CookCounty,IL', 'Jurisdiction_DadeCounty,FL', 'Jurisdiction_DallasCounty,TX', 'Jurisdiction_Denver,CO', 'Jurisdiction_ErieCounty,NY', 'Jurisdiction_FranklinCounty,OH', 'Jurisdiction_HarrisCounty,TX', 'Jurisdiction_HennepinCounty,MN', 'Jurisdiction_Honolulucounty,HI', 'Jurisdiction_JeffersonCounty,KY', 'Jurisdiction_KingCounty,WA', 'Jurisdiction_KingsCounty,NY', 'Jurisdiction_LosAngelesCounty,CA', 'Jurisdiction_MaricopaCounty,AZ', 'Jurisdiction_MilwaukeeCounty,WI', 'Jurisdiction_MonroeCounty,NY', 'Jurisdiction_NassauCounty,NY', 'Jurisdiction_NewYorkCounty,NY', 'Jurisdiction_OklahomaCounty,OK', 'Jurisdiction_OrangeCounty,CA', 'Jurisdiction_Philadelphia,PA', 'Jurisdiction_SanBernardinoCounty,CA', 'Jurisdiction_SanDiegoCounty,CA', 'Jurisdiction_SanFrancisco,CA', 'Jurisdiction_SantaClaraCounty,CA', 'Jurisdiction_StLouisCity,MO', 'Jurisdiction_StLouisCounty,MO', 'Jurisdiction_SuffolkCounty,NY', 'Jurisdiction_VenturaCounty,CA', 'Presentenceinvestigationbeforesentencing_No', 'Presentenceinvestigationbeforesentencing_Notascertained', 'Presentenceinvestigationbeforesentencing_Yes', 'Probationrecommended_No', 'Probationrecommended_Notascertained', 'Probationrecommended_Yes', 'Stillonprobation_No', 'Stillonprobation_Notascertained', 'Stillonprobation_Yes', 'Initialsupervisionlevel_Administrativesupervision', 'Initialsupervisionlevel_Intensivesupervision', 'Initialsupervisionlevel_Lowsupervision', 'Initialsupervisionlevel_Maximumsupervision', 'Initialsupervisionlevel_Mediumsupervision', 'Initialsupervisionlevel_Minimumsupervision', 'Initialsupervisionlevel_Notascertained', 'Lastsupervisionlevel_Administrativesupervision', 'Lastsupervisionlevel_Intensivesupervision', 'Lastsupervisionlevel_Maximumsupervision', 'Lastsupervisionlevel_Mediumsupervision', 'Lastsupervisionlevel_Minimumsupervision', 'Lastsupervisionlevel_Notascertained', 'Lastsupervisionlevel.1_Notascertained', 'Lastsupervisionlevel.1_Nothing,notreatment/testing', 'Lastsupervisionlevel.1_Testingonly', 'Lastsupervisionlevel.1_Treatmentonly', 'Lastsupervisionlevel.1_Treatment/testingimposed', 'Arrestdatematch-rapsheetandsentencingsurvey_No', 'Arrestdatematch-rapsheetandsentencingsurvey_Notascertained', 'Arrestdatematch-rapsheetandsentencingsurvey_Yes', 'Numberofconvictioncharges_geq1q1.0', 'Numberofconvictioncharges_geq2q1.0', 'Numberofconvictioncharges_geq3q1.0', 'Numberofconvictioncharges_geq4q1.0', 'Averagehourlywage_geq1q57.0', 'Averagehourlywage_geq2q100.0', 'Averagehourlywage_geq3q998.0', 'Averagehourlywage_geq4q998.0', 'Numberofprobationofficers_geq1q1.0', 'Numberofprobationofficers_geq2q2.0', 'Numberofprobationofficers_geq3q3.0', 'Numberofprobationofficers_geq4q98.0', 'Caseloadofprobationofficers_geq1q185.0', 'Caseloadofprobationofficers_geq2q9998.0', 'Caseloadofprobationofficers_geq3q9998.0', 'Caseloadofprobationofficers_geq4q9998.0', 'Communityorpublicservicehoursordered_geq1q9999.0', 'Communityorpublicservicehoursordered_geq2q9999.0', 'Communityorpublicservicehoursordered_geq3q9999.0', 'Communityorpublicservicehoursordered_geq4q9999.0', 'Numberofhoursperformedtodate_geq1q9999.0', 'Numberofhoursperformedtodate_geq2q9999.0', 'Numberofhoursperformedtodate_geq3q9999.0', 'Numberofhoursperformedtodate_geq4q9999.0', 'Percentofcommunityworkperformed_geq1q0.0', 'Percentofcommunityworkperformed_geq2q0.0', 'Percentofcommunityworkperformed_geq3q0.0', 'Percentofcommunityworkperformed_geq4q999.0', 'Numberofbehavioralconditionsimposed_geq1q0.0', 'Numberofbehavioralconditionsimposed_geq2q1.0', 'Numberofbehavioralconditionsimposed_geq3q1.0', 'Numberofbehavioralconditionsimposed_geq4q4.0', 'Scoreforcomplyingwithbehavioralconditionsimposed_geq1q0.0', 'Scoreforcomplyingwithbehavioralconditionsimposed_geq2q0.0', 'Scoreforcomplyingwithbehavioralconditionsimposed_geq3q0.0', 'Scoreforcomplyingwithbehavioralconditionsimposed_geq4q1.0', 'Percentofcompliancewithbehavioralconditions_geq1q38.0', 'Percentofcompliancewithbehavioralconditions_geq2q100.0', 'Percentofcompliancewithbehavioralconditions_geq3q999.0', 'Percentofcompliancewithbehavioralconditions_geq4q999.0', 'Probationcaseweight_geq1q2.0', 'Probationcaseweight_geq2q3.0', 'Probationcaseweight_geq3q7.0', 'Probationcaseweight_geq4q10.0', 'Lengthofprobationsentence_geq1q24.0', 'Lengthofprobationsentence_geq2q36.0', 'Lengthofprobationsentence_geq3q48.0', 'Lengthofprobationsentence_geq4q60.0', 'Weightedprobationfeesassessed_geq1q198.0', 'Weightedprobationfeesassessed_geq2q396.0', 'Weightedprobationfeesassessed_geq3q792.0', 'Weightedprobationfeesassessed_geq4q1980.0', 'Weightedprobationfeespaid_geq1q99.0', 'Weightedprobationfeespaid_geq2q297.0', 'Weightedprobationfeespaid_geq3q594.0', 'Weightedprobationfeespaid_geq4q1386.0', 'Weightedrestitutionassessment_geq1q198.0', 'Weightedrestitutionassessment_geq2q396.0', 'Weightedrestitutionassessment_geq3q792.0', 'Weightedrestitutionassessment_geq4q1980.0', 'Weightedrestitutionpayment_geq1q99.0', 'Weightedrestitutionpayment_geq2q297.0', 'Weightedrestitutionpayment_geq3q693.0', 'Weightedrestitutionpayment_geq4q1386.0', 'Totalfinancialconditionsimposed_geq1q0.0', 'Totalfinancialconditionsimposed_geq2q1.0', 'Totalfinancialconditionsimposed_geq3q2.0', 'Totalfinancialconditionsimposed_geq4q3.0', 'Weightedtotalassessment_geq1q1386.0', 'Weightedtotalassessment_geq2q3268.8', 'Weightedtotalassessment_geq3q6266.799999999998', 'Weightedtotalassessment_geq4q13860.0', 'Weightedtotalpayment_geq1q1190.0', 'Weightedtotalpayment_geq2q2376.0', 'Weightedtotalpayment_geq3q4158.0', 'Weightedtotalpayment_geq4q8817.200000000003', 'Amountfineassessmentimposed_geq1q99.0', 'Amountfineassessmentimposed_geq2q99.0', 'Amountfineassessmentimposed_geq3q99.0', 'Amountfineassessmentimposed_geq4q99.0', 'Amountcourtfeesassessed_geq1q99.0', 'Amountcourtfeesassessed_geq2q99.0', 'Amountcourtfeesassessed_geq3q99.0', 'Amountcourtfeesassessed_geq4q99.0', 'Amountdefenderfeesassessed_geq1q99.0', 'Amountdefenderfeesassessed_geq2q99.0', 'Amountdefenderfeesassessed_geq3q99.0', 'Amountdefenderfeesassessed_geq4q99.0', 'Weightedcourtassessments_geq1q594.0', 'Weightedcourtassessments_geq2q1188.0', 'Weightedcourtassessments_geq3q2299.199999999999', 'Weightedcourtassessments_geq4q4480.0', 'Weightedcourtpayments_geq1q526.0', 'Weightedcourtpayments_geq2q891.0', 'Weightedcourtpayments_geq3q1782.0', 'Weightedcourtpayments_geq4q3558.000000000002', 'Weightedotherassessments_geq1q338.0', 'Weightedotherassessments_geq2q594.0', 'Weightedotherassessments_geq3q1188.0', 'Weightedotherassessments_geq4q1980.0', 'Weightedotherpayments_geq1q199.0', 'Weightedotherpayments_geq2q557.6000000000004', 'Weightedotherpayments_geq3q990.0', 'Weightedotherpayments_geq4q1980.0', 'Dayssince1900sentence_geq1q31497.0', 'Dayssince1900sentence_geq2q31567.0', 'Dayssince1900sentence_geq3q31642.0', 'Dayssince1900sentence_geq4q31712.0', 'Age-weighted_geq1q48.0', 'Age-weighted_geq2q94.0', 'Age-weighted_geq3q176.0', 'Age-weighted_geq4q336.0', 'Dayssince1900probationbegan_geq1q31490.0', 'Dayssince1900probationbegan_geq2q31562.0', 'Dayssince1900probationbegan_geq3q31639.0', 'Dayssince1900probationbegan_geq4q31714.0', 'Dayssince1900probationended_geq1q32081.2', 'Dayssince1900probationended_geq2q32400.0', 'Dayssince1900probationended_geq3q32713.6', 'Dayssince1900probationended_geq4q99998.0', 'Dayssincefirstdisciplinaryhearing_geq1q32066.0', 'Dayssincefirstdisciplinaryhearing_geq2q99997.0', 'Dayssincefirstdisciplinaryhearing_geq3q99997.0', 'Dayssincefirstdisciplinaryhearing_geq4q99997.0', 'Dayssincefirstviolation_geq1q31931.0', 'Dayssincefirstviolation_geq2q99997.0', 'Dayssincefirstviolation_geq3q99997.0', 'Dayssincefirstviolation_geq4q99997.0', 'Weightedprobationtime_geq1q1210.0', 'Weightedprobationtime_geq2q2430.0', 'Weightedprobationtime_geq3q4764.799999999996', 'Weightedprobationtime_geq4q9100.0', 'Weightedtimefromprobationtofirsthearing_geq1q2223.2', 'Weightedtimefromprobationtofirsthearing_geq2q999998.0', 'Weightedtimefromprobationtofirsthearing_geq3q999998.0', 'Weightedtimefromprobationtofirsthearing_geq4q999998.0', 'Weightedtimeprobationtofirstviolation-days_geq1q1674.6000000000001', 'Weightedtimeprobationtofirstviolation-days_geq2q999998.0', 'Weightedtimeprobationtofirstviolation-days_geq3q999998.0', 'Weightedtimeprobationtofirstviolation-days_geq4q999998.0', 'Riskscore_geq1q0.0', 'Riskscore_geq2q0.0', 'Riskscore_geq3q3.0', 'Riskscore_geq4q6.0', 'Weightedrisk_geq1q10.0', 'Weightedrisk_geq2q35.0', 'Weightedrisk_geq3q998.0', 'Weightedrisk_geq4q998.0']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig_train.privileged_protected_attributes, \n",
    "      dataset_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric for original training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.149242\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\edb\\languagepack\\v2\\Python-3.9\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator Lasso from version 0.22.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression classifier and predictions\n",
    "scale_orig = StandardScaler()\n",
    "X_train = scale_orig.fit_transform(dataset_orig_train.features)\n",
    "y_train = dataset_orig_train.labels.ravel()\n",
    "\n",
    "lmod = pickle.load(open('experiments/recidivism'+str(K)+'_sex_bmodel.pkl','rb'))['clf']\n",
    "y_train_pred = lmod.predict(X_train)\n",
    "\n",
    "dataset_orig_train_pred = dataset_orig_train.copy(deepcopy=True)\n",
    "dataset_orig_train_pred.labels = y_train_pred.reshape(-1,1)\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(0.5-x))\n",
    "dataset_orig_train_pred.scores = sigmoid(y_train_pred).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.17618501, -0.51549429, -0.08486126, ..., -0.0388194 ,\n",
       "        1.35044478,  0.10097131])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain scores for validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "X_test = scale_orig.transform(dataset_orig_test_pred.features)\n",
    "y_test = dataset_orig_test_pred.labels\n",
    "dataset_orig_test_pred.scores = sigmoid(lmod.predict(X_test)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the optimal parameters from the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best threshold for classification only (no fairness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best balanced accuracy (no fairness constraints) = 0.7642\n",
      "Optimal classification threshold (no fairness constraints) = 0.4753\n"
     ]
    }
   ],
   "source": [
    "num_thresh = 100\n",
    "ba_arr = np.zeros(num_thresh)\n",
    "class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "    \n",
    "    fav_inds = dataset_orig_train_pred.scores > class_thresh\n",
    "    dataset_orig_train_pred.labels[fav_inds] = dataset_orig_train_pred.favorable_label\n",
    "    dataset_orig_train_pred.labels[~fav_inds] = dataset_orig_train_pred.unfavorable_label\n",
    "    \n",
    "    classified_metric_orig_train = ClassificationMetric(dataset_orig_train,\n",
    "                                             dataset_orig_train_pred, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "    \n",
    "    ba_arr[idx] = 0.5*(classified_metric_orig_train.true_positive_rate()\\\n",
    "                       +classified_metric_orig_train.true_negative_rate())\n",
    "\n",
    "best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "best_class_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "print(\"Best balanced accuracy (no fairness constraints) = %.4f\" % np.max(ba_arr))\n",
    "print(\"Optimal classification threshold (no fairness constraints) = %.4f\" % best_class_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate optimal parameters for the ROC method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                 privileged_groups=privileged_groups, \n",
    "                                 low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                  num_class_thresh=100, num_ROC_margin=50,\n",
    "                                  metric_name=metric_name,\n",
    "                                  metric_ub=metric_ub, metric_lb=metric_lb)\n",
    "ROC = ROC.fit(dataset_orig_train, dataset_orig_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal classification threshold (with fairness constraints) = 0.2971\n",
      "Optimal ROC margin = 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC.classification_threshold)\n",
    "print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions from Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### train set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Raw predictions - No fairness constraints, only maximizing balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.7642\n",
      "Statistical parity difference = 0.2220\n",
      "Disparate impact = 1.7904\n",
      "Average odds difference = 0.1629\n",
      "Equal opportunity difference = 0.2036\n",
      "Theil index = 0.1283\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the test set\n",
    "fav_inds = dataset_orig_train_pred.scores > best_class_thresh\n",
    "dataset_orig_train_pred.labels[fav_inds] = dataset_orig_train_pred.favorable_label\n",
    "dataset_orig_train_pred.labels[~fav_inds] = dataset_orig_train_pred.unfavorable_label\n",
    "\n",
    "display(Markdown(\"#### train set\"))\n",
    "display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "metric_train_bef = compute_metrics(dataset_orig_train, dataset_orig_train_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### train set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Transformed predictions - With fairness constraints"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.5717\n",
      "Statistical parity difference = 0.1548\n",
      "Disparate impact = 1.2029\n",
      "Average odds difference = 0.1078\n",
      "Equal opportunity difference = 0.0471\n",
      "Theil index = 0.0620\n"
     ]
    }
   ],
   "source": [
    "# Transform the validation set\n",
    "dataset_transf_train_pred = ROC.predict(dataset_orig_train_pred)\n",
    "\n",
    "display(Markdown(\"#### train set\"))\n",
    "display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "metric_train_aft = compute_metrics(dataset_orig_train, dataset_transf_train_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing: Check if the metric optimized has not become worse\n",
    "assert np.abs(metric_train_aft[metric_name]) <= np.abs(metric_train_bef[metric_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions from Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Raw predictions - No fairness constraints, only maximizing balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.7562\n",
      "Statistical parity difference = 0.1655\n",
      "Disparate impact = 1.6272\n",
      "Average odds difference = 0.1324\n",
      "Equal opportunity difference = 0.1714\n",
      "Theil index = 0.1536\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the test set\n",
    "fav_inds = dataset_orig_test_pred.scores > best_class_thresh\n",
    "dataset_orig_test_pred.labels[fav_inds] = dataset_orig_test_pred.favorable_label\n",
    "dataset_orig_test_pred.labels[~fav_inds] = dataset_orig_test_pred.unfavorable_label\n",
    "\n",
    "display(Markdown(\"#### Test set\"))\n",
    "display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "metric_test_bef = compute_metrics(dataset_orig_test, dataset_orig_test_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Transformed predictions - With fairness constraints"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.5812\n",
      "Statistical parity difference = 0.1143\n",
      "Disparate impact = 1.1450\n",
      "Average odds difference = 0.0792\n",
      "Equal opportunity difference = 0.0243\n",
      "Theil index = 0.0610\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the transformed test set\n",
    "dataset_transf_test_pred = ROC.predict(dataset_orig_test_pred)\n",
    "\n",
    "display(Markdown(\"#### Test set\"))\n",
    "display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "metric_test_aft = compute_metrics(dataset_orig_test, dataset_transf_test_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Error for the test dataset is 0.5103\n",
      "The Equal opportunity difference for the test dataset is 0.02426\n"
     ]
    }
   ],
   "source": [
    "#自己计算error, 不是balanced accuracy！！！\n",
    "metric_test_aft[\"Equal opportunity difference\"]\n",
    "print(\"The Error for the test dataset is {:.4}\".format(np.mean(dataset_orig_test.labels!=dataset_transf_test_pred.labels)))\n",
    "print(\"The Equal opportunity difference for the test dataset is {:.4}\".format(metric_test_aft[\"Equal opportunity difference\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
