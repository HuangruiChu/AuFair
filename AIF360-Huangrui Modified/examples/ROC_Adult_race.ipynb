{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook demonstrates the use of the Reject Option Classification (ROC) post-processing algorithm for bias mitigation for Adult race.\n",
    "- The debiasing function used is implemented in the `RejectOptionClassification` class.\n",
    "- Divide the dataset into training, validation, and testing partitions.\n",
    "- Train classifier on original training data.\n",
    "- Estimate the optimal classification threshold, that maximizes balanced accuracy without fairness constraints.\n",
    "- Estimate the optimal classification threshold, and the critical region boundary (ROC margin) using a validation set for the desired constraint on fairness. The best parameters are those that maximize the classification threshold while satisfying the fairness constraints.\n",
    "- The constraints can be used on the following fairness measures:\n",
    "    * Statistical parity difference on the predictions of the classifier\n",
    "    * Average odds difference for the classifier\n",
    "    * Equal opportunity difference for the classifier\n",
    "- Determine the prediction scores for testing data. Using the estimated optimal classification threshold, compute accuracy and fairness metrics.\n",
    "- Using the determined optimal classification threshold and the ROC margin, adjust the predictions. Report accuracy and fairness metric on the new predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from warnings import warn\n",
    "\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import ClassificationMetric, BinaryLabelDatasetMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "from aif360.algorithms.postprocessing.reject_option_classification\\\n",
    "        import RejectOptionClassification\n",
    "from common_utils import compute_metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive, FloatSlider\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and specify options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Huangrui's ADULT PREPROCESS\n",
    "\n",
    "Only modify the sex and race to 1/0\n",
    "\n",
    "With favorable one as 1. \n",
    "\n",
    "keep all other features\n",
    "\n",
    "Race Is not dummy columned, thus we have fewer columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huangrui defined Adult Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_mappings = {\n",
    "    'label_maps': [{1.0: '>50K', 0.0: '<=50K'}],\n",
    "    'protected_attribute_maps': [{1.0: 'Non-Black', 0.0: 'Black'},\n",
    "                                 {1.0: 'Male', 0.0: 'Female'}]\n",
    "}\n",
    "def code_continuous(df,collist,Nlevel):\n",
    "    for col in collist:\n",
    "        for q in range(1,Nlevel,1):\n",
    "            threshold = df[~np.isnan(df[col])][col].quantile(float(q)/Nlevel)\n",
    "            df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
    "    df.drop(collist,axis = 1, inplace = True)\n",
    "class AdultDataset(StandardDataset):\n",
    "    \"\"\"Adult Census Income Dataset.\n",
    "\n",
    "    See :file:`aif360/data/raw/adult/README.md`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, label_name='Y',\n",
    "                 favorable_classes=[1],\n",
    "                 protected_attribute_names=[\"race_Black\"],\n",
    "                 privileged_classes=[ [0]],\n",
    "                 instance_weights_name=None,\n",
    "                 categorical_features=[],\n",
    "                 features_to_keep=[], features_to_drop=[],\n",
    "                 na_values=['?'], custom_preprocessing=None,\n",
    "                 metadata=default_mappings, path=None):\n",
    "         #deal with the race\n",
    "         df = pd.read_csv(\"./Huangrui/adult/adult_train1.csv\",index_col=False,header = 0, sep = ',')\n",
    "         numericals = [col for col in df.columns if len(df[col].unique())>2 and max(df[col])>1]\n",
    "         code_continuous(df,numericals, 5)\n",
    "         df.drop([\"race_Asian-Pac-Islander\",\n",
    "            \"race_Amer-Indian-Eskimo\",\n",
    "            \"race_White\",\n",
    "            \"race_Other\"], axis=1, inplace=True)\n",
    "         super(AdultDataset, self).__init__(df=df, label_name=label_name,\n",
    "            favorable_classes=favorable_classes,\n",
    "            protected_attribute_names=protected_attribute_names,\n",
    "            privileged_classes=privileged_classes,\n",
    "            instance_weights_name=instance_weights_name,\n",
    "            categorical_features=categorical_features,\n",
    "            features_to_keep=features_to_keep,\n",
    "            features_to_drop=features_to_drop, na_values=na_values,\n",
    "            custom_preprocessing=custom_preprocessing, metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import dataset\n",
    "dataset_used = \"adult\" # \"adult\", \"german\", \"compas\"\n",
    "\n",
    "privileged_groups = [{'race_Black': 0}]\n",
    "unprivileged_groups = [{'race_Black': 1}]\n",
    "\n",
    "# Metric used (should be one of allowed_metrics)\n",
    "metric_name = \"Equal opportunity difference\"\n",
    "\n",
    "# Upper and lower bound on the fairness metric used\n",
    "metric_ub = 0.05\n",
    "metric_lb = -0.05\n",
    "        \n",
    "#random seed for calibrated equal odds prediction\n",
    "random_seed = 12345679\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Verify metric name\n",
    "allowed_metrics = [\"Statistical parity difference\",\n",
    "                   \"Average odds difference\",\n",
    "                   \"Equal opportunity difference\"]\n",
    "if metric_name not in allowed_metrics:\n",
    "    raise ValueError(\"Metric name should be one of allowed metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\edb\\languagepack\\v2\\Python-3.9\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator Lasso from version 0.22.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.8185\n",
      "Statistical parity difference = -0.1107\n",
      "Disparate impact = 0.6930\n",
      "Average odds difference = 0.0100\n",
      "Equal opportunity difference = 0.1731\n",
      "Theil index = 0.0813\n",
      "Balanced accuracy = 0.7499\n",
      "Statistical parity difference = -0.1436\n",
      "Disparate impact = 0.7581\n",
      "Average odds difference = -0.0640\n",
      "Equal opportunity difference = 0.0385\n",
      "Theil index = 0.0679\n",
      "Balanced accuracy = 0.7932\n",
      "Statistical parity difference = -0.1767\n",
      "Disparate impact = 0.4577\n",
      "Average odds difference = -0.1011\n",
      "Equal opportunity difference = -0.0994\n",
      "Theil index = 0.0989\n",
      "Balanced accuracy = 0.7666\n",
      "Statistical parity difference = -0.2274\n",
      "Disparate impact = 0.5964\n",
      "Average odds difference = -0.1103\n",
      "Equal opportunity difference = -0.0427\n",
      "Theil index = 0.0710\n",
      "K = 1, budget = 0.01\n",
      "The Error for the test dataset is 0.3278\n",
      "The Equal opportunity difference for the test dataset is -0.04268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\edb\\languagepack\\v2\\Python-3.9\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator Lasso from version 0.22.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.8273\n",
      "Statistical parity difference = -0.0377\n",
      "Disparate impact = 0.8690\n",
      "Average odds difference = 0.0778\n",
      "Equal opportunity difference = 0.2308\n",
      "Theil index = 0.0836\n",
      "Balanced accuracy = 0.8189\n",
      "Statistical parity difference = -0.0829\n",
      "Disparate impact = 0.6441\n",
      "Average odds difference = -0.0227\n",
      "Equal opportunity difference = 0.0385\n",
      "Theil index = 0.0903\n",
      "Balanced accuracy = 0.7779\n",
      "Statistical parity difference = -0.1578\n",
      "Disparate impact = 0.4285\n",
      "Average odds difference = -0.1085\n",
      "Equal opportunity difference = -0.1371\n",
      "Theil index = 0.1106\n",
      "Balanced accuracy = 0.7551\n",
      "Statistical parity difference = -0.1423\n",
      "Disparate impact = 0.3796\n",
      "Average odds difference = -0.1198\n",
      "Equal opportunity difference = -0.1770\n",
      "Theil index = 0.1250\n",
      "K = 2, budget = 0.01\n",
      "The Error for the test dataset is 0.1662\n",
      "The Equal opportunity difference for the test dataset is -0.177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\edb\\languagepack\\v2\\Python-3.9\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator Lasso from version 0.22.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.8300\n",
      "Statistical parity difference = -0.0331\n",
      "Disparate impact = 0.8831\n",
      "Average odds difference = 0.0808\n",
      "Equal opportunity difference = 0.2308\n",
      "Theil index = 0.0825\n",
      "Balanced accuracy = 0.7499\n",
      "Statistical parity difference = -0.1436\n",
      "Disparate impact = 0.7581\n",
      "Average odds difference = -0.0640\n",
      "Equal opportunity difference = 0.0385\n",
      "Theil index = 0.0679\n",
      "Balanced accuracy = 0.7776\n",
      "Statistical parity difference = -0.1555\n",
      "Disparate impact = 0.4330\n",
      "Average odds difference = -0.1047\n",
      "Equal opportunity difference = -0.1312\n",
      "Theil index = 0.1109\n",
      "Balanced accuracy = 0.7665\n",
      "Statistical parity difference = -0.2229\n",
      "Disparate impact = 0.6028\n",
      "Average odds difference = -0.1084\n",
      "Equal opportunity difference = -0.0442\n",
      "Theil index = 0.0715\n",
      "K = 3, budget = 0.01\n",
      "The Error for the test dataset is 0.3269\n",
      "The Equal opportunity difference for the test dataset is -0.04424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\edb\\languagepack\\v2\\Python-3.9\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator Lasso from version 0.22.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.8333\n",
      "Statistical parity difference = -0.0783\n",
      "Disparate impact = 0.6570\n",
      "Average odds difference = -0.0263\n",
      "Equal opportunity difference = 0.0192\n",
      "Theil index = 0.0831\n",
      "Balanced accuracy = 0.8333\n",
      "Statistical parity difference = -0.0783\n",
      "Disparate impact = 0.6570\n",
      "Average odds difference = -0.0263\n",
      "Equal opportunity difference = 0.0192\n",
      "Theil index = 0.0831\n",
      "Balanced accuracy = 0.7552\n",
      "Statistical parity difference = -0.1469\n",
      "Disparate impact = 0.3528\n",
      "Average odds difference = -0.1276\n",
      "Equal opportunity difference = -0.1891\n",
      "Theil index = 0.1250\n",
      "Balanced accuracy = 0.7552\n",
      "Statistical parity difference = -0.1469\n",
      "Disparate impact = 0.3528\n",
      "Average odds difference = -0.1276\n",
      "Equal opportunity difference = -0.1891\n",
      "Theil index = 0.1250\n",
      "K = 4, budget = 0.01\n",
      "The Error for the test dataset is 0.1646\n",
      "The Equal opportunity difference for the test dataset is -0.1891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\edb\\languagepack\\v2\\Python-3.9\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator Lasso from version 0.22.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.8273\n",
      "Statistical parity difference = -0.0377\n",
      "Disparate impact = 0.8690\n",
      "Average odds difference = 0.0778\n",
      "Equal opportunity difference = 0.2308\n",
      "Theil index = 0.0836\n",
      "Balanced accuracy = 0.7472\n",
      "Statistical parity difference = -0.1482\n",
      "Disparate impact = 0.7523\n",
      "Average odds difference = -0.0670\n",
      "Equal opportunity difference = 0.0385\n",
      "Theil index = 0.0679\n",
      "Balanced accuracy = 0.7772\n",
      "Statistical parity difference = -0.1544\n",
      "Disparate impact = 0.4293\n",
      "Average odds difference = -0.1006\n",
      "Equal opportunity difference = -0.1230\n",
      "Theil index = 0.1113\n",
      "Balanced accuracy = 0.7664\n",
      "Statistical parity difference = -0.2242\n",
      "Disparate impact = 0.6008\n",
      "Average odds difference = -0.1076\n",
      "Equal opportunity difference = -0.0407\n",
      "Theil index = 0.0714\n",
      "K = 5, budget = 0.01\n",
      "The Error for the test dataset is 0.3271\n",
      "The Equal opportunity difference for the test dataset is -0.04067\n"
     ]
    }
   ],
   "source": [
    "experiments_info = {}\n",
    "budget = 0.01\n",
    "for K in range(1, 6):\n",
    "    dataset_orig_train= AdultDataset(path=\"./Huangrui/adult/adult_train{}.csv\".format(K),protected_attribute_names=['race_Black'],\n",
    "                privileged_classes=[[0]])\n",
    "    dataset_orig_test= AdultDataset(path=\"./Huangrui/adult/adult_test{}.csv\".format(K),protected_attribute_names=['race_Black'],\n",
    "                privileged_classes=[[0]])\n",
    "    #only use the budget% of the training data\n",
    "    dataset_orig_train,_ = dataset_orig_train.split([budget], shuffle=False)\n",
    "\n",
    "    # Lasso linear classifier and predictions\n",
    "    X_train = dataset_orig_train.features\n",
    "    y_train = dataset_orig_train.labels.ravel()\n",
    "    lmod = pickle.load(open('experiments/adult'+str(K)+'_race_bmodel.pkl','rb'))[\"clf\"]\n",
    "   \n",
    "    y_train_pred = lmod.predict(X_train)\n",
    "\n",
    "    dataset_orig_train_pred = dataset_orig_train.copy(deepcopy=True)\n",
    "    dataset_orig_train_pred.labels = y_train_pred.reshape(-1,1)\n",
    "    sigmoid = lambda x: 1 / (1 + np.exp(0.5-x))\n",
    "    dataset_orig_train_pred.scores = sigmoid(y_train_pred).reshape(-1,1)\n",
    "\n",
    "    dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "    X_test = dataset_orig_test_pred.features\n",
    "    y_test = dataset_orig_test_pred.labels\n",
    "    dataset_orig_test_pred.scores = sigmoid(lmod.predict(X_test)).reshape(-1,1)\n",
    "\n",
    "    #### Best threshold for classification only (no fairness)\n",
    "    num_thresh = 100\n",
    "    ba_arr = np.zeros(num_thresh)\n",
    "    class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "    for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "        \n",
    "        fav_inds = dataset_orig_train_pred.scores > class_thresh\n",
    "        dataset_orig_train_pred.labels[fav_inds] = dataset_orig_train_pred.favorable_label\n",
    "        dataset_orig_train_pred.labels[~fav_inds] = dataset_orig_train_pred.unfavorable_label\n",
    "        \n",
    "        classified_metric_orig_train = ClassificationMetric(dataset_orig_train,\n",
    "                                                dataset_orig_train_pred, \n",
    "                                                unprivileged_groups=unprivileged_groups,\n",
    "                                                privileged_groups=privileged_groups)\n",
    "        \n",
    "        ba_arr[idx] = 0.5*(classified_metric_orig_train.true_positive_rate()\\\n",
    "                        +classified_metric_orig_train.true_negative_rate())\n",
    "\n",
    "    best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "    best_class_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "    # print(\"Best balanced accuracy (no fairness constraints) = %.4f\" % np.max(ba_arr))\n",
    "    # print(\"Optimal classification threshold (no fairness constraints) = %.4f\" % best_class_thresh)\n",
    "    #### Estimate optimal parameters for the ROC method\n",
    "    ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                    privileged_groups=privileged_groups, \n",
    "                                    low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                    num_class_thresh=100, num_ROC_margin=50,\n",
    "                                    metric_name=metric_name,\n",
    "                                    metric_ub=metric_ub, metric_lb=metric_lb)\n",
    "    ROC = ROC.fit(dataset_orig_train, dataset_orig_train_pred)\n",
    "    # print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC.classification_threshold)\n",
    "    # print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)\n",
    "\n",
    "    # Metrics for the train set\n",
    "    fav_inds = dataset_orig_train_pred.scores > best_class_thresh\n",
    "    dataset_orig_train_pred.labels[fav_inds] = dataset_orig_train_pred.favorable_label\n",
    "    dataset_orig_train_pred.labels[~fav_inds] = dataset_orig_train_pred.unfavorable_label\n",
    "\n",
    "    # display(Markdown(\"#### train set\"))\n",
    "    # display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "    metric_train_bef = compute_metrics(dataset_orig_train, dataset_orig_train_pred, \n",
    "                    unprivileged_groups, privileged_groups)\n",
    "\n",
    "    # Transform the validation set\n",
    "    dataset_transf_train_pred = ROC.predict(dataset_orig_train_pred)\n",
    "\n",
    "    # display(Markdown(\"#### train set\"))\n",
    "    # display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "    metric_train_aft = compute_metrics(dataset_orig_train, dataset_transf_train_pred, \n",
    "                    unprivileged_groups, privileged_groups)\n",
    "    # Testing: Check if the metric optimized has not become worse\n",
    "    assert np.abs(metric_train_aft[metric_name]) <= np.abs(metric_train_bef[metric_name])\n",
    "\n",
    "    # Metrics for the test set\n",
    "    fav_inds = dataset_orig_test_pred.scores > best_class_thresh\n",
    "    dataset_orig_test_pred.labels[fav_inds] = dataset_orig_test_pred.favorable_label\n",
    "    dataset_orig_test_pred.labels[~fav_inds] = dataset_orig_test_pred.unfavorable_label\n",
    "\n",
    "    # display(Markdown(\"#### Test set\"))\n",
    "    # display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "    metric_test_bef = compute_metrics(dataset_orig_test, dataset_orig_test_pred, \n",
    "                    unprivileged_groups, privileged_groups)\n",
    "    # Metrics for the transformed test set\n",
    "    dataset_transf_test_pred = ROC.predict(dataset_orig_test_pred)\n",
    "\n",
    "    # display(Markdown(\"#### Test set\"))\n",
    "    # display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "    metric_test_aft = compute_metrics(dataset_orig_test, dataset_transf_test_pred, \n",
    "                    unprivileged_groups, privileged_groups)\n",
    "\n",
    "    #自己计算error, 不是balanced accuracy！！！\n",
    "    print(\"K = {}, budget = {}\".format(K, budget))\n",
    "    print(\"The Error for the test dataset is {:.4}\".format(np.mean(dataset_orig_test.labels!=dataset_transf_test_pred.labels)))\n",
    "    print(\"The Equal opportunity difference for the test dataset is {:.4}\".format(metric_test_aft[\"Equal opportunity difference\"]))\n",
    "    experiments_info[\"K = {}, budget = {}\".format(K, budget)] = {\"Error\": np.mean(dataset_orig_test.labels!=dataset_transf_test_pred.labels), \"Equal opportunity difference\": metric_test_aft[\"Equal opportunity difference\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'K = 1, budget = 0.01': {'Error': 0.32780532522188427,\n",
       "  'Equal opportunity difference': -0.04268318204410493},\n",
       " 'K = 2, budget = 0.01': {'Error': 0.1661735905662736,\n",
       "  'Equal opportunity difference': -0.17699400946974653},\n",
       " 'K = 3, budget = 0.01': {'Error': 0.32693028876203173,\n",
       "  'Equal opportunity difference': -0.04424032751719986},\n",
       " 'K = 4, budget = 0.01': {'Error': 0.1645901912579691,\n",
       "  'Equal opportunity difference': -0.18908232401182407},\n",
       " 'K = 5, budget = 0.01': {'Error': 0.32713863077628236,\n",
       "  'Equal opportunity difference': -0.040668898945771303}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
