{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from warnings import warn\n",
    "import pandas as pd\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import ClassificationMetric, BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.postprocessing.reject_option_classification\\\n",
    "        import RejectOptionClassification\n",
    "from common_utils import compute_metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive, FloatSlider\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huangrui's financial-inclusion-in-africa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_continuous(df,collist,Nlevel):\n",
    "    for col in collist:\n",
    "        for q in range(1,Nlevel,1):\n",
    "            threshold = df[~np.isnan(df[col])][col].quantile(float(q)/Nlevel)\n",
    "            df[col+'_geq'+str(int(q))+'q'+str(threshold)] = (df[col] >= threshold).astype(float)\n",
    "    df.drop(collist,axis = 1, inplace = True)\n",
    "    \n",
    "class Zindi(StandardDataset):\n",
    "    \"\"\"financial-inclusion-in-africa dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, label_name='Y', favorable_classes=[1],  \n",
    "                 protected_attribute_names=['sex'],\n",
    "                 privileged_classes=[[1]],\n",
    "                 instance_weights_name=None,\n",
    "                 categorical_features=[],\n",
    "                 features_to_drop=[],\n",
    "                 features_to_keep=[],\n",
    "                 na_values=[], custom_preprocessing=None,\n",
    "                 metadata=None):\n",
    "        \"\"\"See :obj:`RegressionDataset` for a description of the arguments.\"\"\"\n",
    "        \n",
    "\n",
    "        df = pd.read_csv(path)\n",
    "        numericals = [col for col in df.columns if len(df[col].unique())>2 and max(df[col])>1]\n",
    "        code_continuous(df,numericals, 5)\n",
    "\n",
    "        super(Zindi, self).__init__(\n",
    "            df=df, label_name=label_name,\n",
    "            favorable_classes=favorable_classes,\n",
    "            protected_attribute_names=protected_attribute_names,\n",
    "            privileged_classes=privileged_classes,\n",
    "            instance_weights_name=instance_weights_name,\n",
    "            categorical_features=categorical_features,\n",
    "            features_to_keep=features_to_keep,\n",
    "            features_to_drop=features_to_drop, na_values=na_values,\n",
    "            custom_preprocessing=custom_preprocessing, metadata=metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and specify options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]\n",
    "# Metric used (should be one of allowed_metrics)\n",
    "metric_name = \"Equal opportunity difference\"\n",
    "#random seed for calibrated equal odds prediction\n",
    "random_seed = 12345679\n",
    "np.random.seed(random_seed)\n",
    "# Upper and lower bound on the fairness metric used\n",
    "metric_ub = 0.05\n",
    "metric_lb = -0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.8054\n",
      "Statistical parity difference = -0.2974\n",
      "Disparate impact = 0.3264\n",
      "Average odds difference = -0.1831\n",
      "Equal opportunity difference = -0.1333\n",
      "Theil index = 0.0742\n",
      "Balanced accuracy = 0.8295\n",
      "Statistical parity difference = -0.1744\n",
      "Disparate impact = 0.5203\n",
      "Average odds difference = -0.0625\n",
      "Equal opportunity difference = -0.0333\n",
      "Theil index = 0.0666\n",
      "Balanced accuracy = 0.7598\n",
      "Statistical parity difference = -0.2122\n",
      "Disparate impact = 0.4009\n",
      "Average odds difference = -0.1881\n",
      "Equal opportunity difference = -0.2226\n",
      "Theil index = 0.0868\n",
      "Balanced accuracy = 0.7464\n",
      "Statistical parity difference = -0.1053\n",
      "Disparate impact = 0.6366\n",
      "Average odds difference = -0.0546\n",
      "Equal opportunity difference = -0.0508\n",
      "Theil index = 0.0909\n",
      "K = 1, budget = 0.01\n",
      "The Error for the test dataset is 0.1854\n",
      "The Equal opportunity difference for the test dataset is -0.05078\n",
      "Balanced accuracy = 0.7580\n",
      "Statistical parity difference = -0.2471\n",
      "Disparate impact = 0.3778\n",
      "Average odds difference = -0.2081\n",
      "Equal opportunity difference = -0.2143\n",
      "Theil index = 0.0893\n",
      "Balanced accuracy = 0.7049\n",
      "Statistical parity difference = -0.3588\n",
      "Disparate impact = 0.5564\n",
      "Average odds difference = -0.1955\n",
      "Equal opportunity difference = 0.0000\n",
      "Theil index = 0.0695\n",
      "Balanced accuracy = 0.7479\n",
      "Statistical parity difference = -0.1426\n",
      "Disparate impact = 0.5486\n",
      "Average odds difference = -0.0991\n",
      "Equal opportunity difference = -0.0801\n",
      "Theil index = 0.0897\n",
      "Balanced accuracy = 0.6875\n",
      "Statistical parity difference = -0.3004\n",
      "Disparate impact = 0.6183\n",
      "Average odds difference = -0.1950\n",
      "Equal opportunity difference = -0.0759\n",
      "Theil index = 0.0673\n",
      "K = 2, budget = 0.01\n",
      "The Error for the test dataset is 0.4871\n",
      "The Equal opportunity difference for the test dataset is -0.07591\n",
      "Balanced accuracy = 0.6991\n",
      "Statistical parity difference = -0.2362\n",
      "Disparate impact = 0.4022\n",
      "Average odds difference = -0.2615\n",
      "Equal opportunity difference = -0.3273\n",
      "Theil index = 0.0829\n",
      "Balanced accuracy = 0.6250\n",
      "Statistical parity difference = -0.3151\n",
      "Disparate impact = 0.6685\n",
      "Average odds difference = -0.1626\n",
      "Equal opportunity difference = 0.0000\n",
      "Theil index = 0.0417\n",
      "Balanced accuracy = 0.7498\n",
      "Statistical parity difference = -0.2312\n",
      "Disparate impact = 0.5126\n",
      "Average odds difference = -0.1758\n",
      "Equal opportunity difference = -0.1618\n",
      "Theil index = 0.0860\n",
      "Balanced accuracy = 0.6059\n",
      "Statistical parity difference = -0.2256\n",
      "Disparate impact = 0.7555\n",
      "Average odds difference = -0.1360\n",
      "Equal opportunity difference = -0.0335\n",
      "Theil index = 0.0491\n",
      "K = 3, budget = 0.01\n",
      "The Error for the test dataset is 0.6567\n",
      "The Equal opportunity difference for the test dataset is -0.03347\n",
      "Balanced accuracy = 0.7868\n",
      "Statistical parity difference = -0.3940\n",
      "Disparate impact = 0.4053\n",
      "Average odds difference = -0.1979\n",
      "Equal opportunity difference = 0.0551\n",
      "Theil index = 0.0725\n",
      "Balanced accuracy = 0.7245\n",
      "Statistical parity difference = -0.3801\n",
      "Disparate impact = 0.5173\n",
      "Average odds difference = -0.2172\n",
      "Equal opportunity difference = -0.0037\n",
      "Theil index = 0.0702\n",
      "Balanced accuracy = 0.7425\n",
      "Statistical parity difference = -0.4065\n",
      "Disparate impact = 0.3540\n",
      "Average odds difference = -0.3131\n",
      "Equal opportunity difference = -0.2302\n",
      "Theil index = 0.0827\n",
      "Balanced accuracy = 0.7071\n",
      "Statistical parity difference = -0.4242\n",
      "Disparate impact = 0.4456\n",
      "Average odds difference = -0.3158\n",
      "Equal opportunity difference = -0.2040\n",
      "Theil index = 0.0763\n",
      "K = 4, budget = 0.01\n",
      "The Error for the test dataset is 0.4172\n",
      "The Equal opportunity difference for the test dataset is -0.204\n",
      "Balanced accuracy = 0.7204\n",
      "Statistical parity difference = -0.3537\n",
      "Disparate impact = 0.4558\n",
      "Average odds difference = -0.2219\n",
      "Equal opportunity difference = -0.1000\n",
      "Theil index = 0.0908\n",
      "Balanced accuracy = 0.7091\n",
      "Statistical parity difference = -0.2778\n",
      "Disparate impact = 0.6296\n",
      "Average odds difference = -0.1500\n",
      "Equal opportunity difference = -0.0333\n",
      "Theil index = 0.0698\n",
      "Balanced accuracy = 0.7312\n",
      "Statistical parity difference = -0.2015\n",
      "Disparate impact = 0.6267\n",
      "Average odds difference = -0.1125\n",
      "Equal opportunity difference = -0.0320\n",
      "Theil index = 0.0854\n",
      "Balanced accuracy = 0.7024\n",
      "Statistical parity difference = -0.2950\n",
      "Disparate impact = 0.5985\n",
      "Average odds difference = -0.1950\n",
      "Equal opportunity difference = -0.0908\n",
      "Theil index = 0.0730\n",
      "K = 5, budget = 0.01\n",
      "The Error for the test dataset is 0.4412\n",
      "The Equal opportunity difference for the test dataset is -0.0908\n"
     ]
    }
   ],
   "source": [
    "experiments_info = {}\n",
    "budget = 0.01\n",
    "for K in range(1, 6):\n",
    "    dataset_orig_train= Zindi(path=\"./Huangrui/zindi/zindi_train{}.csv\".format(K),protected_attribute_names=['sex'],\n",
    "                privileged_classes=[[1]])\n",
    "    dataset_orig_test= Zindi(path=\"./Huangrui/zindi/zindi_test{}.csv\".format(K),protected_attribute_names=['sex'],\n",
    "                privileged_classes=[[1]])\n",
    "    #only use the budget% of the training data\n",
    "    dataset_orig_train,_ = dataset_orig_train.split([budget], shuffle=False)\n",
    "\n",
    "    # Lasso linear classifier and predictions\n",
    "    X_train = dataset_orig_train.features\n",
    "    y_train = dataset_orig_train.labels.ravel()\n",
    "    lmod = pickle.load(open('experiments/zindi'+str(K)+'_sex_bmodel.pkl','rb'))[\"clf\"]\n",
    "   \n",
    "    y_train_pred = lmod.predict(X_train)\n",
    "\n",
    "    dataset_orig_train_pred = dataset_orig_train.copy(deepcopy=True)\n",
    "    dataset_orig_train_pred.labels = y_train_pred.reshape(-1,1)\n",
    "    sigmoid = lambda x: 1 / (1 + np.exp(0.5-x))\n",
    "    dataset_orig_train_pred.scores = sigmoid(y_train_pred).reshape(-1,1)\n",
    "\n",
    "    dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "    X_test = dataset_orig_test_pred.features\n",
    "    y_test = dataset_orig_test_pred.labels\n",
    "    dataset_orig_test_pred.scores = sigmoid(lmod.predict(X_test)).reshape(-1,1)\n",
    "\n",
    "    #### Best threshold for classification only (no fairness)\n",
    "    num_thresh = 100\n",
    "    ba_arr = np.zeros(num_thresh)\n",
    "    class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "    for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "        \n",
    "        fav_inds = dataset_orig_train_pred.scores > class_thresh\n",
    "        dataset_orig_train_pred.labels[fav_inds] = dataset_orig_train_pred.favorable_label\n",
    "        dataset_orig_train_pred.labels[~fav_inds] = dataset_orig_train_pred.unfavorable_label\n",
    "        \n",
    "        classified_metric_orig_train = ClassificationMetric(dataset_orig_train,\n",
    "                                                dataset_orig_train_pred, \n",
    "                                                unprivileged_groups=unprivileged_groups,\n",
    "                                                privileged_groups=privileged_groups)\n",
    "        \n",
    "        ba_arr[idx] = 0.5*(classified_metric_orig_train.true_positive_rate()\\\n",
    "                        +classified_metric_orig_train.true_negative_rate())\n",
    "\n",
    "    best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "    best_class_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "    # print(\"Best balanced accuracy (no fairness constraints) = %.4f\" % np.max(ba_arr))\n",
    "    # print(\"Optimal classification threshold (no fairness constraints) = %.4f\" % best_class_thresh)\n",
    "    #### Estimate optimal parameters for the ROC method\n",
    "    ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                    privileged_groups=privileged_groups, \n",
    "                                    low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                    num_class_thresh=100, num_ROC_margin=50,\n",
    "                                    metric_name=metric_name,\n",
    "                                    metric_ub=metric_ub, metric_lb=metric_lb)\n",
    "    ROC = ROC.fit(dataset_orig_train, dataset_orig_train_pred)\n",
    "    # print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC.classification_threshold)\n",
    "    # print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)\n",
    "\n",
    "    # Metrics for the train set\n",
    "    fav_inds = dataset_orig_train_pred.scores > best_class_thresh\n",
    "    dataset_orig_train_pred.labels[fav_inds] = dataset_orig_train_pred.favorable_label\n",
    "    dataset_orig_train_pred.labels[~fav_inds] = dataset_orig_train_pred.unfavorable_label\n",
    "\n",
    "    # display(Markdown(\"#### train set\"))\n",
    "    # display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "    metric_train_bef = compute_metrics(dataset_orig_train, dataset_orig_train_pred, \n",
    "                    unprivileged_groups, privileged_groups)\n",
    "\n",
    "    # Transform the validation set\n",
    "    dataset_transf_train_pred = ROC.predict(dataset_orig_train_pred)\n",
    "\n",
    "    # display(Markdown(\"#### train set\"))\n",
    "    # display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "    metric_train_aft = compute_metrics(dataset_orig_train, dataset_transf_train_pred, \n",
    "                    unprivileged_groups, privileged_groups)\n",
    "    # Testing: Check if the metric optimized has not become worse\n",
    "    assert np.abs(metric_train_aft[metric_name]) <= np.abs(metric_train_bef[metric_name])\n",
    "\n",
    "    # Metrics for the test set\n",
    "    fav_inds = dataset_orig_test_pred.scores > best_class_thresh\n",
    "    dataset_orig_test_pred.labels[fav_inds] = dataset_orig_test_pred.favorable_label\n",
    "    dataset_orig_test_pred.labels[~fav_inds] = dataset_orig_test_pred.unfavorable_label\n",
    "\n",
    "    # display(Markdown(\"#### Test set\"))\n",
    "    # display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "    metric_test_bef = compute_metrics(dataset_orig_test, dataset_orig_test_pred, \n",
    "                    unprivileged_groups, privileged_groups)\n",
    "    # Metrics for the transformed test set\n",
    "    dataset_transf_test_pred = ROC.predict(dataset_orig_test_pred)\n",
    "\n",
    "    # display(Markdown(\"#### Test set\"))\n",
    "    # display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "    metric_test_aft = compute_metrics(dataset_orig_test, dataset_transf_test_pred, \n",
    "                    unprivileged_groups, privileged_groups)\n",
    "\n",
    "    #自己计算error, 不是balanced accuracy！！！\n",
    "    print(\"K = {}, budget = {}\".format(K, budget))\n",
    "    print(\"The Error for the test dataset is {:.4}\".format(np.mean(dataset_orig_test.labels!=dataset_transf_test_pred.labels)))\n",
    "    print(\"The Equal opportunity difference for the test dataset is {:.4}\".format(metric_test_aft[\"Equal opportunity difference\"]))\n",
    "    experiments_info[\"K = {}, budget = {}\".format(K, budget)] = {\"Error\": np.mean(dataset_orig_test.labels!=dataset_transf_test_pred.labels), \"Equal opportunity difference\": metric_test_aft[\"Equal opportunity difference\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'K = 1, budget = 0.01': {'Error': 0.18537414965986396,\n",
       "  'Equal opportunity difference': -0.050778543558326894},\n",
       " 'K = 2, budget = 0.01': {'Error': 0.4871413390010627,\n",
       "  'Equal opportunity difference': -0.07591075554581406},\n",
       " 'K = 3, budget = 0.01': {'Error': 0.6567481402763018,\n",
       "  'Equal opportunity difference': -0.03347288333237752},\n",
       " 'K = 4, budget = 0.01': {'Error': 0.4172157279489904,\n",
       "  'Equal opportunity difference': -0.20398388809843482},\n",
       " 'K = 5, budget = 0.01': {'Error': 0.4412327311370882,\n",
       "  'Equal opportunity difference': -0.09080234833659495}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
